{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "original_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pulkitsingh/Pokedex/blob/master/original_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TKrJRlUqyjz",
        "colab_type": "text"
      },
      "source": [
        "### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S621NksmcZlQ",
        "colab_type": "code",
        "outputId": "987beef5-85eb-4f9d-8981-644c94b41066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcTUKBfMbo2J",
        "colab_type": "code",
        "outputId": "778eb691-3e7b-4dad-ecde-bf50fc6ee561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gdrive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDurK1mBq1-i",
        "colab_type": "text"
      },
      "source": [
        "### Importing Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wGBTYGmdkCH",
        "colab_type": "code",
        "outputId": "3e7d3f21-1ab7-4423-d7c2-376a0cade30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D, GlobalAveragePooling2D, BatchNormalization, Input, Conv2D, MaxPooling2D, Dropout, Flatten\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQstzZLbq50l",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIRzIGWTdytP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# directories for training and validation data\n",
        "train_directory = 'original_data_clean/train'\n",
        "validation_directory = 'original_data_clean/validation'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zaref9b8d1oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making ImageDataGenerator instances for training and test data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, \n",
        "                                   zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is2nABHMt6GV",
        "colab_type": "code",
        "outputId": "4a1e8e5a-d0d6-423e-d421-1a8f1bdf388a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# loading the train and validation data\n",
        "train_gen = train_datagen.flow_from_directory('original_data_clean/train',\n",
        "                                              target_size=(224, 224),\n",
        "                                              batch_size=32,\n",
        "                                              color_mode=\"rgb\",                                            \n",
        "                                              class_mode='categorical', \n",
        "                                              shuffle=True, seed=333)\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory('original_data_clean/validation',\n",
        "                                            target_size=(224, 224),\n",
        "                                            color_mode = \"rgb\",\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='categorical', \n",
        "                                            shuffle=True, seed=333)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8478 images belonging to 149 classes.\n",
            "Found 2189 images belonging to 149 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mAUrGGS5oyM",
        "colab_type": "text"
      },
      "source": [
        "### Specifying the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg4TfXI3qkO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specifying parameters\n",
        "batch_size = 10\n",
        "input_shape = (10, 224, 224, 3)\n",
        "num_training_samples = 12903\n",
        "num_test_samples = 3317"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JJu8mq4LbWz",
        "colab_type": "code",
        "outputId": "c89de73b-30f1-4b11-b02c-520940e29102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# specifying the model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=(224, 224, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(149))\n",
        "model.add(Activation('softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2TXw4wQLbsG",
        "colab_type": "code",
        "outputId": "e7a60f01-b024-4a65-dc5e-a513af565b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile(loss=categorical_crossentropy, optimizer=adam, metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 224, 224, 32)      896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 224, 224, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 222, 222, 32)      9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 222, 222, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 111, 111, 64)      18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 111, 111, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 109, 109, 64)      36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 109, 109, 64)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 186624)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               95552000  \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 149)               76437     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 149)               0         \n",
            "=================================================================\n",
            "Total params: 95,694,005\n",
            "Trainable params: 95,694,005\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S8-I2rY3waz",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkZFSp3l38Di",
        "colab_type": "code",
        "outputId": "d2b5485b-39f7-4eed-94a3-84c601c79918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# making a directory to save weights\n",
        "import os\n",
        "dir_name = 'pokemon_weights/original2'\n",
        "#os.mkdir(dir_name)\n",
        "path = os.path.join(os.getcwd(), dir_name)\n",
        "print(path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/pokemon_weights/original2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_WE653E52ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = os.path.join(path, \"model-{epoch:02d}.hdf5\")\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, \n",
        "                             save_best_only=False, save_weights_only=True, \n",
        "                             mode='auto', period=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYrp7gOmDH5P",
        "colab_type": "code",
        "outputId": "6c9ee45f-5d4e-4cab-cc02-72d3d8eae05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit_generator(train_gen, \n",
        "                    epochs=100,\n",
        "                    workers=10,\n",
        "                    use_multiprocessing=True,\n",
        "                    shuffle=True,\n",
        "                    callbacks=[checkpoint],\n",
        "                    validation_data=test_gen,\n",
        "                    validation_steps=1, \n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 77/265 [=======>......................] - ETA: 5:05 - loss: 4.0600 - acc: 0.1404"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "175/265 [==================>...........] - ETA: 2:19 - loss: 4.0320 - acc: 0.1438"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 1s - loss: 4.0431 - acc: 0.1401\n",
            "265/265 [==============================] - 398s 2s/step - loss: 4.0431 - acc: 0.1401 - val_loss: 3.8122 - val_acc: 0.1875\n",
            "\n",
            "Epoch 00001: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-01.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/100\n",
            "202/265 [=====================>........] - ETA: 18s - loss: 3.9162 - acc: 0.1556"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 282ms/step - loss: 3.9049 - acc: 0.1547 - val_loss: 3.8602 - val_acc: 0.1875\n",
            "\n",
            "Epoch 00002: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-02.hdf5\n",
            "Epoch 3/100\n",
            " 86/265 [========>.....................] - ETA: 56s - loss: 3.7508 - acc: 0.1781"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "223/265 [========================>.....] - ETA: 12s - loss: 3.7364 - acc: 0.1836"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 3.7287 - acc: 0.1878Epoch 3/100\n",
            "265/265 [==============================] - 77s 291ms/step - loss: 3.7285 - acc: 0.1880 - val_loss: 3.3222 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00003: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-03.hdf5\n",
            "Epoch 4/100\n",
            " 27/265 [==>...........................] - ETA: 1:27 - loss: 3.6682 - acc: 0.1898"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "143/265 [===============>..............] - ETA: 35s - loss: 3.6117 - acc: 0.2022"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "179/265 [===================>..........] - ETA: 25s - loss: 3.5986 - acc: 0.2042"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 3.5857 - acc: 0.2083\n",
            "265/265 [==============================] - 77s 290ms/step - loss: 3.5854 - acc: 0.2084 - val_loss: 3.5264 - val_acc: 0.2500\n",
            "\n",
            "Epoch 00004: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-04.hdf5\n",
            "Epoch 5/100\n",
            " 45/265 [====>.........................] - ETA: 1:07 - loss: 3.5484 - acc: 0.2174"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "112/265 [===========>..................] - ETA: 46s - loss: 3.4656 - acc: 0.2344"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 288ms/step - loss: 3.3962 - acc: 0.2459 - val_loss: 3.7409 - val_acc: 0.1875\n",
            "\n",
            "Epoch 00005: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-05.hdf5\n",
            "Epoch 6/100\n",
            " 35/265 [==>...........................] - ETA: 1:29 - loss: 3.2279 - acc: 0.2598"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "215/265 [=======================>......] - ETA: 14s - loss: 3.1995 - acc: 0.2768"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 3.1857 - acc: 0.2817\n",
            "265/265 [==============================] - 77s 291ms/step - loss: 3.1846 - acc: 0.2815 - val_loss: 3.2111 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00006: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-06.hdf5\n",
            "Epoch 7/100\n",
            " 57/265 [=====>........................] - ETA: 1:06 - loss: 2.9721 - acc: 0.3103"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "192/265 [====================>.........] - ETA: 21s - loss: 3.0036 - acc: 0.3050"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 77s 291ms/step - loss: 3.1846 - acc: 0.2815 - val_loss: 3.2111 - val_acc: 0.4375\n",
            "Epoch 7/100\n",
            "265/265 [==============================] - 75s 285ms/step - loss: 3.0087 - acc: 0.3071 - val_loss: 3.2849 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00007: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-07.hdf5\n",
            "Epoch 8/100\n",
            " 29/265 [==>...........................] - ETA: 1:23 - loss: 2.7342 - acc: 0.3502"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "157/265 [================>.............] - ETA: 31s - loss: 2.8346 - acc: 0.3419"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 285ms/step - loss: 2.8291 - acc: 0.3438 - val_loss: 3.3841 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00008: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-08.hdf5\n",
            "Epoch 9/100\n",
            " 14/265 [>.............................] - ETA: 1:42 - loss: 2.5885 - acc: 0.4040"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 92/265 [=========>....................] - ETA: 52s - loss: 2.6591 - acc: 0.3835"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 283ms/step - loss: 2.6725 - acc: 0.3731 - val_loss: 3.2400 - val_acc: 0.2500\n",
            "\n",
            "Epoch 00009: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-09.hdf5\n",
            "Epoch 10/100\n",
            "218/265 [=======================>......] - ETA: 13s - loss: 2.5557 - acc: 0.3925"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "227/265 [========================>.....] - ETA: 10s - loss: 2.5595 - acc: 0.3915"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 279ms/step - loss: 2.5518 - acc: 0.3939 - val_loss: 2.7523 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-10.hdf5\n",
            "Epoch 11/100\n",
            "249/265 [===========================>..] - ETA: 4s - loss: 2.4172 - acc: 0.4179"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 2.4150 - acc: 0.4190\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-10.hdf5\n",
            "265/265 [==============================] - 78s 294ms/step - loss: 2.4170 - acc: 0.4187 - val_loss: 3.9328 - val_acc: 0.2500\n",
            "\n",
            "Epoch 00011: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-11.hdf5\n",
            "Epoch 12/100\n",
            " 89/265 [=========>....................] - ETA: 51s - loss: 2.3222 - acc: 0.4463"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "235/265 [=========================>....] - ETA: 8s - loss: 2.3135 - acc: 0.4485"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 280ms/step - loss: 2.3234 - acc: 0.4457 - val_loss: 3.3875 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00012: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-12.hdf5\n",
            "Epoch 13/100\n",
            " 65/265 [======>.......................] - ETA: 56s - loss: 2.1677 - acc: 0.4749"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "162/265 [=================>............] - ETA: 29s - loss: 2.1507 - acc: 0.4758"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 278ms/step - loss: 2.1796 - acc: 0.4721 - val_loss: 3.7174 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00013: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-13.hdf5\n",
            "Epoch 14/100\n",
            "180/265 [===================>..........] - ETA: 24s - loss: 2.0916 - acc: 0.4876"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "196/265 [=====================>........] - ETA: 19s - loss: 2.1022 - acc: 0.4863"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 2.1436 - acc: 0.4763\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "265/265 [==============================] - 74s 280ms/step - loss: 2.1440 - acc: 0.4762 - val_loss: 2.3089 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00014: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-14.hdf5\n",
            "Epoch 15/100\n",
            " 53/265 [=====>........................] - ETA: 1:03 - loss: 1.9026 - acc: 0.5342"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 279ms/step - loss: 2.0205 - acc: 0.5054 - val_loss: 2.9514 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00015: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-15.hdf5\n",
            "Epoch 16/100\n",
            "143/265 [===============>..............] - ETA: 34s - loss: 1.9132 - acc: 0.5332"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "235/265 [=========================>....] - ETA: 8s - loss: 1.9208 - acc: 0.5248"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.9283 - acc: 0.5230\n",
            "265/265 [==============================] - 74s 278ms/step - loss: 1.9292 - acc: 0.5224 - val_loss: 2.9651 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00016: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-16.hdf5\n",
            "Epoch 17/100\n",
            " 55/265 [=====>........................] - ETA: 1:06 - loss: 1.8531 - acc: 0.5364"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 73/265 [=======>......................] - ETA: 59s - loss: 1.8542 - acc: 0.5312"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 281ms/step - loss: 1.8571 - acc: 0.5360 - val_loss: 3.2184 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00017: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-17.hdf5\n",
            "Epoch 18/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "117/265 [============>.................] - ETA: 43s - loss: 1.7307 - acc: 0.5562"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 280ms/step - loss: 1.7760 - acc: 0.5451 - val_loss: 3.2856 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00018: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-18.hdf5\n",
            "Epoch 19/100\n",
            "105/265 [==========>...................] - ETA: 48s - loss: 1.6943 - acc: 0.5829"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "110/265 [===========>..................] - ETA: 47s - loss: 1.6824 - acc: 0.5868"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.7043 - acc: 0.5717\n",
            "265/265 [==============================] - 75s 284ms/step - loss: 1.7049 - acc: 0.5715 - val_loss: 2.5234 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00019: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-19.hdf5\n",
            "Epoch 20/100\n",
            " 92/265 [=========>....................] - ETA: 51s - loss: 1.6870 - acc: 0.5751"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "116/265 [============>.................] - ETA: 43s - loss: 1.6777 - acc: 0.5795"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 282ms/step - loss: 1.6806 - acc: 0.5764 - val_loss: 2.8771 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-20.hdf5\n",
            "Epoch 21/100\n",
            " 53/265 [=====>........................] - ETA: 1:09 - loss: 1.5257 - acc: 0.6240"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "194/265 [====================>.........] - ETA: 21s - loss: 1.6125 - acc: 0.5869"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.6095 - acc: 0.5882\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-20.hdf5\n",
            "265/265 [==============================] - 75s 284ms/step - loss: 1.6096 - acc: 0.5881 - val_loss: 2.6191 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00021: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-21.hdf5\n",
            "Epoch 22/100\n",
            "203/265 [=====================>........] - ETA: 18s - loss: 1.5065 - acc: 0.6102"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "222/265 [========================>.....] - ETA: 12s - loss: 1.5237 - acc: 0.6073"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 283ms/step - loss: 1.5292 - acc: 0.6039 - val_loss: 3.1254 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00022: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-22.hdf5\n",
            "Epoch 23/100\n",
            " 29/265 [==>...........................] - ETA: 1:16 - loss: 1.5137 - acc: 0.6261"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 62/265 [======>.......................] - ETA: 1:00 - loss: 1.4847 - acc: 0.6265"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 281ms/step - loss: 1.5187 - acc: 0.6109 - val_loss: 3.5434 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00023: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-23.hdf5\n",
            "Epoch 24/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "108/265 [===========>..................] - ETA: 46s - loss: 1.3994 - acc: 0.6383"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 285ms/step - loss: 1.4426 - acc: 0.6247 - val_loss: 2.8423 - val_acc: 0.4688\n",
            "\n",
            "Epoch 00024: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-24.hdf5\n",
            "Epoch 25/100\n",
            "187/265 [====================>.........] - ETA: 22s - loss: 1.3873 - acc: 0.6372"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "245/265 [==========================>...] - ETA: 5s - loss: 1.4146 - acc: 0.6316"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.4145 - acc: 0.6321\n",
            "Epoch 00024: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-24.hdf5\n",
            "265/265 [==============================] - 75s 282ms/step - loss: 1.4137 - acc: 0.6322 - val_loss: 2.0447 - val_acc: 0.4688\n",
            "\n",
            "Epoch 00025: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-25.hdf5\n",
            "Epoch 26/100\n",
            " 24/265 [=>............................] - ETA: 1:20 - loss: 1.3903 - acc: 0.6341"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "256/265 [===========================>..] - ETA: 2s - loss: 1.3757 - acc: 0.6466"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.3829 - acc: 0.6451\n",
            "265/265 [==============================] - 75s 283ms/step - loss: 1.3819 - acc: 0.6455 - val_loss: 2.7021 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00026: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-26.hdf5\n",
            "Epoch 27/100\n",
            " 68/265 [======>.......................] - ETA: 59s - loss: 1.2760 - acc: 0.6618 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "135/265 [==============>...............] - ETA: 37s - loss: 1.3219 - acc: 0.6512"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 283ms/step - loss: 1.3468 - acc: 0.6469 - val_loss: 3.4542 - val_acc: 0.3438\n",
            "\n",
            "\n",
            "Epoch 00027: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-27.hdf5\n",
            "Epoch 28/100\n",
            " 78/265 [=======>......................] - ETA: 58s - loss: 1.2556 - acc: 0.6651"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "135/265 [==============>...............] - ETA: 38s - loss: 1.2732 - acc: 0.6595"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.2842 - acc: 0.6648Epoch 28/100\n",
            "265/265 [==============================] - 75s 285ms/step - loss: 1.2835 - acc: 0.6649 - val_loss: 2.9478 - val_acc: 0.2812\n",
            "\n",
            "Epoch 00028: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-28.hdf5\n",
            "Epoch 29/100\n",
            " 12/265 [>.............................] - ETA: 1:11 - loss: 1.1973 - acc: 0.6745"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 28/265 [==>...........................] - ETA: 1:10 - loss: 1.2249 - acc: 0.6741"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.2427 - acc: 0.6776\n",
            "Epoch 00028: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-28.hdf5\n",
            "265/265 [==============================] - 75s 282ms/step - loss: 1.2434 - acc: 0.6775 - val_loss: 3.3966 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00029: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-29.hdf5\n",
            "Epoch 30/100\n",
            " 81/265 [========>.....................] - ETA: 56s - loss: 1.1717 - acc: 0.6979"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.2442 - acc: 0.6733\n",
            "Epoch 00029: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-29.hdf5\n",
            "265/265 [==============================] - 75s 285ms/step - loss: 1.2432 - acc: 0.6736 - val_loss: 2.7654 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00030: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-30.hdf5\n",
            "Epoch 31/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 280ms/step - loss: 1.2385 - acc: 0.6770 - val_loss: 3.7956 - val_acc: 0.2500\n",
            "\n",
            "Epoch 00031: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-31.hdf5\n",
            "Epoch 32/100\n",
            "121/265 [============>.................] - ETA: 43s - loss: 1.1853 - acc: 0.6836"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "176/265 [==================>...........] - ETA: 26s - loss: 1.1620 - acc: 0.6907"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.1872 - acc: 0.6839\n",
            "265/265 [==============================] - 76s 285ms/step - loss: 1.1894 - acc: 0.6838 - val_loss: 3.3281 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00032: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-32.hdf5\n",
            "Epoch 33/100\n",
            "150/265 [===============>..............] - ETA: 33s - loss: 1.0767 - acc: 0.7094"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "188/265 [====================>.........] - ETA: 22s - loss: 1.0733 - acc: 0.7070"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 280ms/step - loss: 1.1339 - acc: 0.6915 - val_loss: 3.7444 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00033: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-33.hdf5\n",
            "Epoch 34/100\n",
            " 23/265 [=>............................] - ETA: 1:15 - loss: 1.0876 - acc: 0.7038"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 55/265 [=====>........................] - ETA: 1:03 - loss: 1.0836 - acc: 0.7034"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.1054 - acc: 0.7027\n",
            "265/265 [==============================] - 74s 280ms/step - loss: 1.1040 - acc: 0.7030 - val_loss: 2.8789 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00034: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-34.hdf5\n",
            "Epoch 35/100\n",
            "217/265 [=======================>......] - ETA: 13s - loss: 1.0927 - acc: 0.7085"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "251/265 [===========================>..] - ETA: 3s - loss: 1.1003 - acc: 0.7071"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 280ms/step - loss: 1.1023 - acc: 0.7063 - val_loss: 3.8207 - val_acc: 0.2500\n",
            "\n",
            "Epoch 00035: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-35.hdf5\n",
            "Epoch 36/100\n",
            " 33/265 [==>...........................] - ETA: 1:06 - loss: 1.0188 - acc: 0.7216"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "168/265 [==================>...........] - ETA: 27s - loss: 1.0255 - acc: 0.7251"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 280ms/step - loss: 1.0645 - acc: 0.7147 - val_loss: 2.8788 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00036: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-36.hdf5\n",
            "Epoch 37/100\n",
            " 53/265 [=====>........................] - ETA: 1:07 - loss: 1.1264 - acc: 0.7152"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "242/265 [==========================>...] - ETA: 6s - loss: 1.0763 - acc: 0.7175"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.0732 - acc: 0.7171\n",
            "Epoch 00036: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-36.hdf5\n",
            "265/265 [==============================] - 76s 288ms/step - loss: 1.0720 - acc: 0.7173 - val_loss: 3.2803 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00037: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-37.hdf5\n",
            "Epoch 38/100\n",
            " 13/265 [>.............................] - ETA: 1:48 - loss: 0.9881 - acc: 0.7548"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 69/265 [======>.......................] - ETA: 1:00 - loss: 0.9935 - acc: 0.7360"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 285ms/step - loss: 1.0586 - acc: 0.7180 - val_loss: 4.2857 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00038: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-38.hdf5\n",
            "Epoch 39/100\n",
            "114/265 [===========>..................] - ETA: 45s - loss: 1.0051 - acc: 0.7300"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 1.0008 - acc: 0.7299\n",
            "265/265 [==============================] - 75s 283ms/step - loss: 1.0021 - acc: 0.7296 - val_loss: 2.5639 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00039: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-39.hdf5\n",
            "Epoch 40/100\n",
            " 51/265 [====>.........................] - ETA: 1:04 - loss: 1.0169 - acc: 0.7279"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "113/265 [===========>..................] - ETA: 43s - loss: 0.9959 - acc: 0.7326"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 279ms/step - loss: 0.9778 - acc: 0.7311 - val_loss: 4.1102 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00040: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-40.hdf5\n",
            "Epoch 41/100\n",
            " 61/265 [=====>........................] - ETA: 1:03 - loss: 0.9920 - acc: 0.7218"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "125/265 [=============>................] - ETA: 42s - loss: 0.9710 - acc: 0.7335"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 285ms/step - loss: 0.9981 - acc: 0.7299 - val_loss: 4.4857 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00041: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-41.hdf5\n",
            "Epoch 42/100\n",
            "171/265 [==================>...........] - ETA: 26s - loss: 0.9330 - acc: 0.7438"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "183/265 [===================>..........] - ETA: 23s - loss: 0.9346 - acc: 0.7415"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 280ms/step - loss: 0.9425 - acc: 0.7401 - val_loss: 3.3736 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00042: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-42.hdf5\n",
            "Epoch 43/100\n",
            " 31/265 [==>...........................] - ETA: 1:16 - loss: 0.9687 - acc: 0.7581"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "172/265 [==================>...........] - ETA: 26s - loss: 0.8950 - acc: 0.7574"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 285ms/step - loss: 0.9208 - acc: 0.7541 - val_loss: 2.6641 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00043: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-43.hdf5\n",
            "Epoch 44/100\n",
            "104/265 [==========>...................] - ETA: 48s - loss: 0.9298 - acc: 0.7436"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "178/265 [===================>..........] - ETA: 25s - loss: 0.9281 - acc: 0.7503"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 283ms/step - loss: 0.9371 - acc: 0.7485 - val_loss: 3.2210 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00044: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-44.hdf5\n",
            "Epoch 45/100\n",
            "146/265 [===============>..............] - ETA: 35s - loss: 0.8254 - acc: 0.7743"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "228/265 [========================>.....] - ETA: 10s - loss: 0.8545 - acc: 0.7641"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.8675 - acc: 0.7607\n",
            "Epoch 00044: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-44.hdf5\n",
            "265/265 [==============================] - 76s 286ms/step - loss: 0.8681 - acc: 0.7609 - val_loss: 3.4004 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00045: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-45.hdf5\n",
            "Epoch 46/100\n",
            "109/265 [===========>..................] - ETA: 45s - loss: 0.9127 - acc: 0.7453"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "243/265 [==========================>...] - ETA: 6s - loss: 0.9035 - acc: 0.7484"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.9008 - acc: 0.7504\n",
            "265/265 [==============================] - 75s 284ms/step - loss: 0.9013 - acc: 0.7503 - val_loss: 3.2204 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00046: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-46.hdf5\n",
            "Epoch 47/100\n",
            " 16/265 [>.............................] - ETA: 1:47 - loss: 0.8730 - acc: 0.7500"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.8281 - acc: 0.7741Epoch 47/100\n",
            "265/265 [==============================] - 76s 285ms/step - loss: 0.8306 - acc: 0.7736 - val_loss: 3.3324 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00047: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-47.hdf5\n",
            "Epoch 48/100\n",
            "  5/265 [..............................] - ETA: 3:10 - loss: 0.8183 - acc: 0.7812"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 96/265 [=========>....................] - ETA: 49s - loss: 0.8296 - acc: 0.7669"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.8528 - acc: 0.7645\n",
            "265/265 [==============================] - 75s 284ms/step - loss: 0.8520 - acc: 0.7646 - val_loss: 3.2297 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00048: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-48.hdf5\n",
            "Epoch 49/100\n",
            " 95/265 [=========>....................] - ETA: 49s - loss: 0.8524 - acc: 0.7663"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.8410 - acc: 0.7708\n",
            "Epoch 00048: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-48.hdf5\n",
            "265/265 [==============================] - 75s 283ms/step - loss: 0.8416 - acc: 0.7705 - val_loss: 4.1242 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00049: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-49.hdf5\n",
            "Epoch 50/100\n",
            " 42/265 [===>..........................] - ETA: 1:18 - loss: 0.7844 - acc: 0.7850"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 57/265 [=====>........................] - ETA: 1:06 - loss: 0.7735 - acc: 0.7911"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 283ms/step - loss: 0.8168 - acc: 0.7773 - val_loss: 2.6324 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00050: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-50.hdf5\n",
            "\n",
            "Epoch 51/100\n",
            "132/265 [=============>................] - ETA: 39s - loss: 0.7889 - acc: 0.7810"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "223/265 [========================>.....] - ETA: 12s - loss: 0.7830 - acc: 0.7809"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 286ms/step - loss: 0.7994 - acc: 0.7776 - val_loss: 3.9767 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00051: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-51.hdf5\n",
            "Epoch 52/100\n",
            "132/265 [=============>................] - ETA: 39s - loss: 0.7894 - acc: 0.7801"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "149/265 [===============>..............] - ETA: 34s - loss: 0.7930 - acc: 0.7800"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 283ms/step - loss: 0.8042 - acc: 0.7769 - val_loss: 4.2772 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00052: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-52.hdf5\n",
            "Epoch 53/100\n",
            "128/265 [=============>................] - ETA: 41s - loss: 0.7867 - acc: 0.7836"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "178/265 [===================>..........] - ETA: 25s - loss: 0.7843 - acc: 0.7826"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 286ms/step - loss: 0.8088 - acc: 0.7767 - val_loss: 4.0255 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00053: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-53.hdf5\n",
            "Epoch 54/100\n",
            " 49/265 [====>.........................] - ETA: 1:04 - loss: 0.6871 - acc: 0.8112"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 279ms/step - loss: 0.7957 - acc: 0.7810 - val_loss: 3.9598 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00054: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-54.hdf5\n",
            "Epoch 55/100\n",
            "109/265 [===========>..................] - ETA: 47s - loss: 0.7066 - acc: 0.8050"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "141/265 [==============>...............] - ETA: 36s - loss: 0.7252 - acc: 0.7983"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.7670 - acc: 0.7903\n",
            "Epoch 00054: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-54.hdf5\n",
            "265/265 [==============================] - 76s 286ms/step - loss: 0.7672 - acc: 0.7904 - val_loss: 3.3607 - val_acc: 0.5000\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-55.hdf5\n",
            "Epoch 56/100\n",
            " 67/265 [======>.......................] - ETA: 1:03 - loss: 0.7166 - acc: 0.8050"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "177/265 [===================>..........] - ETA: 25s - loss: 0.7502 - acc: 0.7978"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.7755 - acc: 0.7914Epoch 56/100\n",
            "\n",
            "Epoch 00055: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-55.hdf5\n",
            "265/265 [==============================] - 76s 287ms/step - loss: 0.7751 - acc: 0.7916 - val_loss: 3.3026 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00056: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-56.hdf5\n",
            "Epoch 57/100\n",
            " 74/265 [=======>......................] - ETA: 59s - loss: 0.7285 - acc: 0.7990 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "249/265 [===========================>..] - ETA: 4s - loss: 0.7346 - acc: 0.7964"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.7297 - acc: 0.7973\n",
            "265/265 [==============================] - 76s 287ms/step - loss: 0.7751 - acc: 0.7916 - val_loss: 3.3026 - val_acc: 0.4375\n",
            "265/265 [==============================] - 76s 288ms/step - loss: 0.7304 - acc: 0.7970 - val_loss: 3.2295 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00057: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-57.hdf5\n",
            "Epoch 58/100\n",
            " 62/265 [======>.......................] - ETA: 1:03 - loss: 0.7087 - acc: 0.8115"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "209/265 [======================>.......] - ETA: 16s - loss: 0.7268 - acc: 0.8011"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.7949\n",
            "Epoch 00057: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-57.hdf5\n",
            "265/265 [==============================] - 76s 285ms/step - loss: 0.7439 - acc: 0.7948 - val_loss: 3.4232 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00058: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-58.hdf5\n",
            "Epoch 59/100\n",
            " 70/265 [======>.......................] - ETA: 1:01 - loss: 0.7831 - acc: 0.7790"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "173/265 [==================>...........] - ETA: 27s - loss: 0.7259 - acc: 0.7959"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.7076 - acc: 0.8019\n",
            "Epoch 00058: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-58.hdf5\n",
            "265/265 [==============================] - 75s 285ms/step - loss: 0.7088 - acc: 0.8016 - val_loss: 3.3978 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00059: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-59.hdf5\n",
            "Epoch 60/100\n",
            " 85/265 [========>.....................] - ETA: 52s - loss: 0.7299 - acc: 0.7971"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "253/265 [===========================>..] - ETA: 3s - loss: 0.7250 - acc: 0.7969"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 278ms/step - loss: 0.7267 - acc: 0.7967 - val_loss: 4.8140 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00059: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-59.hdf5\n",
            "\n",
            "Epoch 00060: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-60.hdf5\n",
            "Epoch 61/100\n",
            "204/265 [======================>.......] - ETA: 17s - loss: 0.6899 - acc: 0.8073"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "243/265 [==========================>...] - ETA: 6s - loss: 0.7037 - acc: 0.8032"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 284ms/step - loss: 0.6989 - acc: 0.8044 - val_loss: 2.2121 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00061: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-61.hdf5\n",
            "Epoch 62/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 53/265 [=====>........................] - ETA: 1:07 - loss: 0.6722 - acc: 0.8196"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6703 - acc: 0.8164\n",
            "Epoch 00061: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-61.hdf5\n",
            "265/265 [==============================] - 75s 283ms/step - loss: 0.6691 - acc: 0.8168 - val_loss: 2.5807 - val_acc: 0.4688\n",
            "\n",
            "Epoch 00062: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-62.hdf5\n",
            "Epoch 63/100\n",
            " 64/265 [======>.......................] - ETA: 1:00 - loss: 0.6829 - acc: 0.8179"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 285ms/step - loss: 0.6614 - acc: 0.8187 - val_loss: 2.9804 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00063: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-63.hdf5\n",
            "Epoch 64/100\n",
            "165/265 [=================>............] - ETA: 29s - loss: 0.6540 - acc: 0.8157"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "227/265 [========================>.....] - ETA: 11s - loss: 0.6785 - acc: 0.8136"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6725 - acc: 0.8137\n",
            "Epoch 00063: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-63.hdf5\n",
            "265/265 [==============================] - 75s 284ms/step - loss: 0.6720 - acc: 0.8138 - val_loss: 4.0077 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00064: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-64.hdf5\n",
            "Epoch 65/100\n",
            "166/265 [=================>............] - ETA: 29s - loss: 0.6471 - acc: 0.8194"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "175/265 [==================>...........] - ETA: 26s - loss: 0.6542 - acc: 0.8188"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.8134Epoch 65/100\n",
            "265/265 [==============================] - 76s 286ms/step - loss: 0.6807 - acc: 0.8135 - val_loss: 5.2143 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00064: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-64.hdf5\n",
            "\n",
            "Epoch 00065: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-65.hdf5\n",
            "Epoch 66/100\n",
            "118/265 [============>.................] - ETA: 43s - loss: 0.6673 - acc: 0.8104"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "217/265 [=======================>......] - ETA: 13s - loss: 0.6648 - acc: 0.8142"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6631 - acc: 0.8161Epoch 66/100\n",
            "\n",
            "Epoch 00065: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-65.hdf5\n",
            "265/265 [==============================] - 75s 284ms/step - loss: 0.6618 - acc: 0.8166 - val_loss: 3.8378 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00066: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-66.hdf5\n",
            "Epoch 67/100\n",
            " 25/265 [=>............................] - ETA: 1:08 - loss: 0.6485 - acc: 0.8187"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "213/265 [=======================>......] - ETA: 14s - loss: 0.6710 - acc: 0.8135"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6756 - acc: 0.8114\n",
            "265/265 [==============================] - 74s 280ms/step - loss: 0.6760 - acc: 0.8113 - val_loss: 4.6863 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00067: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-67.hdf5\n",
            "Epoch 68/100\n",
            " 71/265 [=======>......................] - ETA: 57s - loss: 0.6089 - acc: 0.8371"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "102/265 [==========>...................] - ETA: 47s - loss: 0.6020 - acc: 0.8444"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6540 - acc: 0.8248\n",
            "Epoch 00067: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-67.hdf5\n",
            "265/265 [==============================] - 75s 284ms/step - loss: 0.6545 - acc: 0.8245 - val_loss: 3.6902 - val_acc: 0.2812\n",
            "\n",
            "Epoch 00068: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-68.hdf5\n",
            "Epoch 69/100\n",
            " 51/265 [====>.........................] - ETA: 1:09 - loss: 0.6033 - acc: 0.8252"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 86/265 [========>.....................] - ETA: 53s - loss: 0.6250 - acc: 0.8244"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6442 - acc: 0.8225\n",
            "\n",
            "Epoch 00068: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-68.hdf5\n",
            "265/265 [==============================] - 76s 289ms/step - loss: 0.6446 - acc: 0.8225 - val_loss: 3.7757 - val_acc: 0.6154\n",
            "\n",
            "Epoch 00069: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-69.hdf5\n",
            "Epoch 70/100\n",
            "212/265 [=======================>......] - ETA: 15s - loss: 0.6148 - acc: 0.8344"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "217/265 [=======================>......] - ETA: 13s - loss: 0.6115 - acc: 0.8349"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 77s 289ms/step - loss: 0.6133 - acc: 0.8351 - val_loss: 3.5216 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00070: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-70.hdf5\n",
            "Epoch 71/100\n",
            "  3/265 [..............................] - ETA: 4:51 - loss: 0.7759 - acc: 0.8229"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "207/265 [======================>.......] - ETA: 16s - loss: 0.6578 - acc: 0.8244"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6665 - acc: 0.8173\n",
            "Epoch 00070: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-70.hdf5\n",
            "265/265 [==============================] - 75s 285ms/step - loss: 0.6664 - acc: 0.8171 - val_loss: 2.9513 - val_acc: 0.4688\n",
            "\n",
            "Epoch 00071: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-71.hdf5\n",
            "Epoch 72/100\n",
            " 89/265 [=========>....................] - ETA: 54s - loss: 0.6511 - acc: 0.8143"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "167/265 [=================>............] - ETA: 28s - loss: 0.6340 - acc: 0.8192"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 285ms/step - loss: 0.6387 - acc: 0.8197 - val_loss: 4.2297 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00072: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-72.hdf5\n",
            "Epoch 73/100\n",
            " 11/265 [>.............................] - ETA: 1:06 - loss: 0.6323 - acc: 0.8466"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "152/265 [================>.............] - ETA: 31s - loss: 0.6071 - acc: 0.8340"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 280ms/step - loss: 0.6218 - acc: 0.8272 - val_loss: 3.9195 - val_acc: 0.4688\n",
            "\n",
            "Epoch 00073: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-73.hdf5\n",
            "Epoch 74/100\n",
            "189/265 [====================>.........] - ETA: 22s - loss: 0.6322 - acc: 0.8236"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "211/265 [======================>.......] - ETA: 15s - loss: 0.6359 - acc: 0.8218"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6238 - acc: 0.8257\n",
            "Epoch 00073: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-73.hdf5\n",
            "265/265 [==============================] - 76s 286ms/step - loss: 0.6244 - acc: 0.8254 - val_loss: 4.3178 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00074: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-74.hdf5\n",
            "Epoch 75/100\n",
            "111/265 [===========>..................] - ETA: 46s - loss: 0.5594 - acc: 0.8457"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "144/265 [===============>..............] - ETA: 35s - loss: 0.5527 - acc: 0.8465"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 286ms/step - loss: 0.5775 - acc: 0.8407 - val_loss: 3.2801 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00075: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-75.hdf5\n",
            "Epoch 76/100\n",
            "172/265 [==================>...........] - ETA: 27s - loss: 0.5817 - acc: 0.8395"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "175/265 [==================>...........] - ETA: 26s - loss: 0.5794 - acc: 0.8401"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 287ms/step - loss: 0.5819 - acc: 0.8388 - val_loss: 2.1459 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00076: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-76.hdf5\n",
            "Epoch 77/100\n",
            " 36/265 [===>..........................] - ETA: 1:06 - loss: 0.6265 - acc: 0.8220"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 62/265 [======>.......................] - ETA: 1:01 - loss: 0.5978 - acc: 0.8261"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 281ms/step - loss: 0.5829 - acc: 0.8361 - val_loss: 3.4445 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00077: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-77.hdf5\n",
            "Epoch 78/100\n",
            " 63/265 [======>.......................] - ETA: 58s - loss: 0.5748 - acc: 0.8482"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "171/265 [==================>...........] - ETA: 27s - loss: 0.5976 - acc: 0.8376"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.6075 - acc: 0.8341\n",
            "Epoch 00077: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-77.hdf5\n",
            "265/265 [==============================] - 75s 281ms/step - loss: 0.6070 - acc: 0.8344 - val_loss: 4.2104 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00078: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-78.hdf5\n",
            "Epoch 79/100\n",
            "168/265 [==================>...........] - ETA: 28s - loss: 0.5749 - acc: 0.8431"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "228/265 [========================>.....] - ETA: 10s - loss: 0.5778 - acc: 0.8407"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 288ms/step - loss: 0.5788 - acc: 0.8417 - val_loss: 3.5487 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00079: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-79.hdf5\n",
            "Epoch 80/100\n",
            "200/265 [=====================>........] - ETA: 19s - loss: 0.5673 - acc: 0.8418"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "248/265 [===========================>..] - ETA: 4s - loss: 0.5784 - acc: 0.8403"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 286ms/step - loss: 0.5756 - acc: 0.8416 - val_loss: 3.2836 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00080: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-80.hdf5\n",
            "Epoch 81/100\n",
            "213/265 [=======================>......] - ETA: 14s - loss: 0.5535 - acc: 0.8471"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "220/265 [=======================>......] - ETA: 13s - loss: 0.5562 - acc: 0.8466"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.5640 - acc: 0.8448\n",
            "265/265 [==============================] - 76s 286ms/step - loss: 0.5653 - acc: 0.8447 - val_loss: 3.8546 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00081: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-81.hdf5\n",
            "Epoch 82/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "154/265 [================>.............] - ETA: 32s - loss: 0.5775 - acc: 0.8342"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.5759 - acc: 0.8370\n",
            "Epoch 00081: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-81.hdf5\n",
            "265/265 [==============================] - 76s 287ms/step - loss: 0.5763 - acc: 0.8369 - val_loss: 3.1948 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00082: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-82.hdf5\n",
            "Epoch 83/100\n",
            " 24/265 [=>............................] - ETA: 1:13 - loss: 0.5582 - acc: 0.8341"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "202/265 [=====================>........] - ETA: 18s - loss: 0.5669 - acc: 0.8415"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.5845 - acc: 0.8367\n",
            "265/265 [==============================] - 75s 283ms/step - loss: 0.5844 - acc: 0.8366 - val_loss: 4.0622 - val_acc: 0.4688\n",
            "\n",
            "Epoch 00083: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-83.hdf5\n",
            "Epoch 84/100\n",
            "113/265 [===========>..................] - ETA: 46s - loss: 0.5590 - acc: 0.8526"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "133/265 [==============>...............] - ETA: 39s - loss: 0.5447 - acc: 0.8571"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.5736 - acc: 0.8421\n",
            "265/265 [==============================] - 76s 287ms/step - loss: 0.5732 - acc: 0.8418 - val_loss: 4.4293 - val_acc: 0.2188\n",
            "\n",
            "Epoch 00084: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-84.hdf5\n",
            "Epoch 85/100\n",
            "113/265 [===========>..................] - ETA: 44s - loss: 0.5169 - acc: 0.8479"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "202/265 [=====================>........] - ETA: 17s - loss: 0.5445 - acc: 0.8420"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.8460\n",
            "Epoch 00084: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-84.hdf5\n",
            "265/265 [==============================] - 75s 283ms/step - loss: 0.5445 - acc: 0.8462 - val_loss: 2.7226 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00085: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-85.hdf5\n",
            "Epoch 86/100\n",
            "  2/265 [..............................] - ETA: 4:04 - loss: 0.2753 - acc: 0.9375"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 97/265 [=========>....................] - ETA: 47s - loss: 0.5466 - acc: 0.8470"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 283ms/step - loss: 0.5445 - acc: 0.8462 - val_loss: 2.7226 - val_acc: 0.5000\n",
            "265/265 [==============================] - 74s 278ms/step - loss: 0.5871 - acc: 0.8373 - val_loss: 4.2021 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00085: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-85.hdf5\n",
            "\n",
            "Epoch 00086: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-86.hdf5\n",
            "Epoch 87/100\n",
            "184/265 [===================>..........] - ETA: 23s - loss: 0.5369 - acc: 0.8483"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 285ms/step - loss: 0.5436 - acc: 0.8468 - val_loss: 5.5867 - val_acc: 0.2812\n",
            "\n",
            "Epoch 00087: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-87.hdf5\n",
            "Epoch 88/100\n",
            "159/265 [=================>............] - ETA: 30s - loss: 0.5245 - acc: 0.8561"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "220/265 [=======================>......] - ETA: 13s - loss: 0.5353 - acc: 0.8521"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 286ms/step - loss: 0.5384 - acc: 0.8504 - val_loss: 4.0869 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00088: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-88.hdf5\n",
            "Epoch 89/100\n",
            " 63/265 [======>.......................] - ETA: 1:02 - loss: 0.5584 - acc: 0.8532"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "126/265 [=============>................] - ETA: 41s - loss: 0.5213 - acc: 0.8591"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 284ms/step - loss: 0.5285 - acc: 0.8561 - val_loss: 4.7708 - val_acc: 0.4375\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-89.hdf5\n",
            "Epoch 90/100\n",
            "132/265 [=============>................] - ETA: 39s - loss: 0.5357 - acc: 0.8504"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "239/265 [==========================>...] - ETA: 7s - loss: 0.5524 - acc: 0.8448"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 285ms/step - loss: 0.5466 - acc: 0.8469 - val_loss: 4.4414 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00090: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-90.hdf5\n",
            "Epoch 91/100\n",
            " 87/265 [========>.....................] - ETA: 50s - loss: 0.5079 - acc: 0.8560"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "239/265 [==========================>...] - ETA: 7s - loss: 0.5326 - acc: 0.8518"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 74s 280ms/step - loss: 0.5412 - acc: 0.8503 - val_loss: 5.8329 - val_acc: 0.2500\n",
            "\n",
            "Epoch 00091: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-91.hdf5\n",
            "Epoch 92/100\n",
            "100/265 [==========>...................] - ETA: 48s - loss: 0.5468 - acc: 0.8440"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "129/265 [=============>................] - ETA: 40s - loss: 0.5258 - acc: 0.8490"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.8558\n",
            "265/265 [==============================] - 76s 285ms/step - loss: 0.5104 - acc: 0.8558 - val_loss: 6.1239 - val_acc: 0.1562\n",
            "\n",
            "Epoch 00092: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-92.hdf5\n",
            "Epoch 93/100\n",
            " 55/265 [=====>........................] - ETA: 1:06 - loss: 0.5510 - acc: 0.8409"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "219/265 [=======================>......] - ETA: 13s - loss: 0.5244 - acc: 0.8516"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 286ms/step - loss: 0.5349 - acc: 0.8487 - val_loss: 5.0424 - val_acc: 0.1875\n",
            "\n",
            "Epoch 00093: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-93.hdf5\n",
            "Epoch 94/100\n",
            " 74/265 [=======>......................] - ETA: 57s - loss: 0.5103 - acc: 0.8615"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "154/265 [================>.............] - ETA: 32s - loss: 0.5692 - acc: 0.8492"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 282ms/step - loss: 0.5669 - acc: 0.8461 - val_loss: 3.4883 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00094: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-94.hdf5\n",
            "Epoch 95/100\n",
            "129/265 [=============>................] - ETA: 39s - loss: 0.5242 - acc: 0.8583"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "215/265 [=======================>......] - ETA: 14s - loss: 0.5147 - acc: 0.8599"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 75s 282ms/step - loss: 0.5669 - acc: 0.8461 - val_loss: 3.4883 - val_acc: 0.5312\n",
            "265/265 [==============================] - 74s 280ms/step - loss: 0.5198 - acc: 0.8579 - val_loss: 5.6468 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00095: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-95.hdf5\n",
            "Epoch 96/100\n",
            " 50/265 [====>.........................] - ETA: 1:02 - loss: 0.4932 - acc: 0.8681"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "191/265 [====================>.........] - ETA: 21s - loss: 0.5117 - acc: 0.8578"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.8576\n",
            "Epoch 00095: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-95.hdf5\n",
            "265/265 [==============================] - 74s 281ms/step - loss: 0.5171 - acc: 0.8570 - val_loss: 3.8811 - val_acc: 0.5312\n",
            "\n",
            "Epoch 00096: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-96.hdf5\n",
            "Epoch 97/100\n",
            "219/265 [=======================>......] - ETA: 13s - loss: 0.5028 - acc: 0.8570"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "236/265 [=========================>....] - ETA: 8s - loss: 0.5026 - acc: 0.8563"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8568\n",
            "Epoch 00096: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-96.hdf5\n",
            "265/265 [==============================] - 77s 289ms/step - loss: 0.5034 - acc: 0.8569 - val_loss: 3.5733 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00097: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-97.hdf5\n",
            "Epoch 98/100\n",
            "111/265 [===========>..................] - ETA: 46s - loss: 0.5312 - acc: 0.8395"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "205/265 [======================>.......] - ETA: 17s - loss: 0.5321 - acc: 0.8443"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 286ms/step - loss: 0.5230 - acc: 0.8477 - val_loss: 2.6759 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00098: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-98.hdf5\n",
            "Epoch 99/100\n",
            " 51/265 [====>.........................] - ETA: 1:09 - loss: 0.4999 - acc: 0.8572"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 80/265 [========>.....................] - ETA: 56s - loss: 0.5091 - acc: 0.8574"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "265/265 [==============================] - 76s 285ms/step - loss: 0.5017 - acc: 0.8612 - val_loss: 4.4746 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00099: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-99.hdf5\n",
            "Epoch 100/100\n",
            " 58/265 [=====>........................] - ETA: 1:03 - loss: 0.4861 - acc: 0.8680"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "154/265 [================>.............] - ETA: 32s - loss: 0.5164 - acc: 0.8597"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "264/265 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8578\n",
            "Epoch 00099: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-99.hdf5\n",
            "265/265 [==============================] - 76s 285ms/step - loss: 0.5200 - acc: 0.8578 - val_loss: 3.3939 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00100: saving model to /content/gdrive/My Drive/pokemon_weights/original2/model-100.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLFWMcxhGXtY",
        "colab_type": "code",
        "outputId": "3f4b1292-aa86-47f8-b392-4ab516720a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.argmax(history.history['val_acc'])\n",
        "np.max(history.history['val_acc'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6153846383094788"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wLlnvQYJiQI",
        "colab_type": "code",
        "outputId": "d583a8d5-a9a9-47f9-9330-57bcca636f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['acc'], label=\"Training\")\n",
        "plt.plot(history.history['val_acc'], label=\"Validation\", c='purple')\n",
        "plt.ylim((0, 1))\n",
        "plt.ylabel(\"Accuracy (percentage)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.title(\"Classifier Accuracy for Original Dataset\")\n",
        "plt.legend()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f709e47b048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRNy1C_Ir_kb",
        "colab_type": "code",
        "outputId": "f15fa54f-8af4-4fd6-d76b-c43aa100793c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], label=\"Training\")\n",
        "plt.plot(history.history['val_loss'], label=\"Validation\", c='purple')\n",
        "plt.ylabel(\"Loss (Cross-Entropy)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.title(\"Classifier Loss for Original Dataset\")\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f709e339dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydeZhcVZ3+P6f2ru7qvdNJOvtCdkhC\nCCC7IBAWQUGFARlxYUQHdBjHYUYcl8dRf46DqOOGKKCyiCCKrMpO2EISYshKQjpb793ptbqqazu/\nP26d27f2W9VV6U5yP89TT9JVt27d6q5673vf8z3fI6SUWFhYWFgcfdjG+wAsLCwsLEqDJfAWFhYW\nRymWwFtYWFgcpVgCb2FhYXGUYgm8hYWFxVGKJfAWFhYWRymWwB9BCCG+LoT4XQn3v1UIcXb8/0II\ncbcQolcIsU4IcYYQYmepXvtwIoT4kBDigBBiSAixYhyP4xohxF+Lva2Jfe0VQpxXjH1ZTGwsgZ9g\nCCH+QQixPi4+bUKIp4QQpx+O15ZSLpFSvhj/8XTgA8A0KeVqKeUrUsoFxXotIcQ9QohvFWt/efJ9\n4J+llBVSyreLsUMhxCXxE6FfCNEjhLhPCDEt23OklPdJKc83s/98th0L8b9LSAgxGL9tEUJ8RwhR\nlcc+DssJxDpR5cYS+AmEEOIW4A7g20AjMAP4KXDZOBzOTGCvlNI/1h0JIRxFOJ5iMhPYWsgThRD2\nNPddCdyP9rerB5YAI8BaIURNhv1MtN+Jke9JKX1AA3A9cArwqhCifHwPyyJvpJTWbQLcgCpgCPhI\nlm2+DvzO8PMfgHagH3gZWGJ47CJgGzAItABfit9fDzwO9AGHgFcAW/yxvcB5wKeAIBCNH9M3gLOB\ng4b9TwUeAbqAZuDmpON8GPgdMAB8Os17uQf4Vob3+T7grfj7egt4n+GxTwB74u+rGbgmfv884KX4\nc7qB36fZrzv+fiTgB96L378IeDH+O9kKfDDpOH8GPBl/znlJ+xTAPuDLSffbgC3ANw3H/SrwA6AH\n+Fb8vrWG55wP7Iy/h5/G38+nDc83biuBzwK74sf9E0DEH5sLPB9/nW7gPqDa8Ny9ye8j298F8AFt\naFc9WfcP/BaIAYH47/rLhX5W449dAmyKv8fXgOOzvY51S/p7jvcBWLf4HwIuBCKAI8s2XydR4D8Z\n//K50dzjJsNjbcAZ8f/XACvj//8O8HPAGb+dYRAG/YufRlDOJi7wcfHaAPwX4ALmoInuBYbjDAOX\nx7ctS/NeUoQkfn8t0At8HHAAV8d/rgPK0U4YC+LbTlFCATwAfCX+eh7g9Cy/RwnMi//fCewG/jP+\nXt4fF5oFhuPsB05T+07a18L4/maneZ1vAK8bfp8R4Kb4+yoz/o7RTrwDwIfjj38h/jvMJvCPA9Vo\nV3pdwIXxx+ahxWtuNBf+MnCH4bn63zmPv8tviJ80C9k/hX1WVwCdwMmAHfjH+L7dud6HddNuVkQz\ncagDuqWUEbNPkFL+Wko5KKUcQRPVEwxZaRhYLISolFL2Sik3Gu6fAsyUUoallq3n25DoJKBBSvlN\nKWVISrkH+CVwlWGb16WUf5JSxqSUgTz2fTGwS0r5WyllREr5ALADuDT+eAxYKoQok1K2SSlV1BJG\ni16mSimDUsq1Jl/vFKAC+G78vTyPJpxXG7b5s5Ty1fh7CSY9vz7+b1uafbcZHgdolVL+OP6+kn8n\nFwFbpZR/jH8GfoTmeLPxXSlln5RyP/ACsBxASrlbSvk3KeWIlLILuB04K8e+ctGKdvItaP8FflZv\nAH4hpXxTShmVUt6LFn2dMsb3csxgCfzEoQeoN5vNCiHsQojvCiHeE0IMoLkZGBWUK9BEY58Q4iUh\nxKnx+/8HzbH+VQixRwhxawHHOhOYKoToUzc0B9xo2OZAAfsFLfrZl3TfPqBJauMBH0OLJtqEEE8I\nIRbGt/kyWlyyLl4N9Mk8Xu+AlDKW/HqGn7O9l+74v1PSPDbF8Hiu/Uw1Ph4/6R7Msj0kngCG0U5U\nCCEahRAPCiFa4p+N35F4oimEJrRIL+/9j+GzOhP416TP2XS035WFCSyBnzi8juZOLje5/T+gDb6e\nh5bfz4rfLwCklG9JKS8DJgF/Ah6K3z8opfxXKeUc4IPALUKIc/M81gNAs5Sy2nDzSSkvMmxTaJvS\nVrQvtpEZaNksUspnpJQfQBPPHWhXDkgp26WUn5FSTgX+CfipEGKeydebLoQwfhf014uT7b3sRBPi\njxjvjO/vCuA5k/tpA/SqGyGEMP6cJ9+Ov9YyKWUlcC3xz0UhCCEq0D5nr5jcf/L7LOizivY5+++k\nz5k3flWX7nUskrAEfoIgpexHy7R/IoS4XAjhFUI4hRBrhBDfS/MUH9oJoQfwon3pABBCuOJ101VS\nyjBathuLP3aJEGJeXED60QZSYyl7z846YFAI8e9CiLK4Q1sqhDgpz/3YhRAew82FNph5XLxc1CGE\n+BiwGHg87hwvi1dzjKANrqn39RFDWWIv2pffzPt6E839fjn++z4bLQ560MwbiDvtLwG3xY/ZI4SY\nDNwFVKINqprhCWBZ/G/vAD4PTDb53GR8aL+bfiFEE/BvhexECOEWQpyIJrq9wN0m99+BNi5jPJ68\nP6toJ+/PCiFOFhrlQoiLhRC+DK9jkYQl8BMIKeX/ArcAt6ENmh0A/hntC5bMb9CihBa0CoQ3kh7/\nOLA3fkn8WeCa+P3zgWfRvqCvAz+VUr6Q53FG0aoblqNVsnSjCZrpWuk4t6JVQajb81LKnvi+/xVN\nEL4MXCKl7Eb7vN6C5roPoeW+N8b3dRLwphBiCHgM+EJ8bCDXewmhCfqa+Pv4KXCdlHKH2Tchpfw9\n2u/7X+LHvA1tEPW0+Psxs49utKuA78X3sRhYjyaM+fINYCXaCfwJ4I95Pv/LQojB+HH8Bm1A/X1y\ntGQ21/6/g3bC6xNCfIkCP6tSyvXAZ4D/QzvB7EYbbM70OhZJqOoJCwuLCUQ84jmIVgaa1wnYwkJh\nOXgLiwmCEOICIUS1EMKNNmgtSHW7FhamsQTewmLicCrwHlpUdClweZ4lphYWCVgRjYWFhcVRiuXg\nLSwsLI5SJlTDo/r6ejlr1qzxPgwLCwuLI4YNGzZ0Sykb0j02oQR+1qxZrF+/frwPw8LCwuKIQQiR\nPPNbx4poLCwsLI5SLIG3sLCwOEqxBN7CwsLiKGVCZfDpCIfDHDx4kGAwuUurRaF4PB6mTZuG0+kc\n70OxsLAoIRNe4A8ePIjP52PWrFlo/bEsxoKUkp6eHg4ePMjs2bPH+3AsLCxKyISPaILBIHV1dZa4\nFwkhBHV1ddYVkYXFMcCEF3jAEvciY/0+LSyODY4IgbewsLAYD2LRGBt/tZFwIDzeh1IQlsBnoaen\nh+XLl7N8+XImT55MU1OT/nMoFDK1j+uvv56dO3dm3eYnP/kJ9913XzEO2cLCoojsenIXf/n0X9j1\n5K7xPpSCKOkgqxCiGm0hiKVoK+x8Ukr5eilfs5jU1dWxadMmAL7+9a9TUVHBl76UuK6Avnq5Lf25\n8u677057v5HPf/7zYz9YCwuLovPuX94FwN/hz7HlxKTUDv6HwNNSyoXACcD2Er/eYWH37t0sXryY\na665hiVLltDW1sYNN9zAqlWrWLJkCd/85jf1bU8//XQ2bdpEJBKhurqaW2+9lRNOOIFTTz2Vzs5O\nAG677TbuuOMOfftbb72V1atXs2DBAl577TUA/H4/V1xxBYsXL+bKK69k1apV+snHwsKi+MiY5N3H\n4wLfdWQKfMkcvBCiCjiT+BJb8aXRzOUaGfjGX7ayrXVg7AdnYPHUSr526ZK8n7djxw5+85vfsGrV\nKgC++93vUltbSyQS4ZxzzuHKK69k8eLFCc/p7+/nrLPO4rvf/S633HILv/71r7n11ltT9i2lZN26\ndTz22GN885vf5Omnn+bHP/4xkydP5pFHHuHvf/87K1euLOwNW1hYmKJtYxtDbUMADHcPj/PRFEYp\nHfxstHVF7xZCvC2EuCu+WPJRwdy5c3VxB3jggQdYuXIlK1euZPv27Wzbti3lOWVlZaxZswaAE088\nkb1796bd94c//OGUbdauXctVV10FwAknnMCSJfmflCwsLMyz8y87ETZB+aRyhruOTIEvZQbvQFuY\n9yYp5ZtCiB+iLbL8VeNGQogbgBsAZsyYkXWHhTjtUlFePnqu2rVrFz/84Q9Zt24d1dXVXHvttWnr\nzF0ul/5/u91OJBJJu2+3251zGwsLi9Ly7l/eZdqp0wCOWIEvpYM/CByUUr4Z//lhNMFPQEp5p5Ry\nlZRyVUND2pbGE56BgQF8Ph+VlZW0tbXxzDPPFP01TjvtNB566CEA3nnnnbRXCBYWFsVh4OAA7W+3\nc9ylx+Gt91oZfDJSynYhxAEhxAIp5U7gXOCoVKWVK1eyePFiFi5cyMyZMznttNOK/ho33XQT1113\nHYsXL9ZvVVVVRX8dCwsL9MHVBZcu4NDuQ7SsaxnnIyqMkq7JKoRYjlYm6QL2ANdLKXszbb9q1SqZ\nvODH9u3bWbRoUcmO8UghEokQiUTweDzs2rWL888/n127duFwFHaOtn6vFhaZuf/i++ne0c1Nu2/i\n+a88z2vff43bRm6bkLPAhRAbpJSr0j1W0jp4KeUmIO0LW+TH0NAQ5557LpFIBCklv/jFLwoWdwsL\ni8yE/CH2PLeHVZ9dhRACb72XWDjGSP8InmrPeB9eXlgKcYRQXV3Nhg0bxvswLCyOelrWtRAdiTLv\nwnkAeBu8gFYqeaQJvNWqwMLCwsKAqnmvnFYJQHmDVjF3JA60WgJvYWFhYSDYp5U4K7furY87+COw\nVNISeAsLCwsDKQJviGiONCyBt7CwsDAQ7Asi7AJnubakpRXRHMWcc845KROX7rjjDm688caMz6mo\nqACgtbWVK6+8Mu02Z599NsklocnccccdDA+PuoaLLrqIvr4+s4duYWFRAMG+IJ5qj14S6fQ6cZQ5\nrIjmaOTqq6/mwQcfTLjvwQcf5Oqrr8753KlTp/Lwww8X/NrJAv/kk09SXV1d8P4sLCxyM9KXWg5Z\n3lCeNaJ58RsvcuD1A6U+tLyxBD4HV155JU888YS+wMfevXtpbW1lxYoVnHvuuaxcuZJly5bx5z//\nOeW5e/fuZenSpQAEAgGuuuoqFi1axIc+9CECgYC+3Y033qi3Gv7a174GwI9+9CNaW1s555xzOOec\ncwCYNWsW3d3dANx+++0sXbqUpUuX6q2G9+7dy6JFi/jMZz7DkiVLOP/88xNex8LCIjfKwRvxNngz\nOngpJS9/82W2PrT1cBxeXhxRdfBPf/Fp2je1F3Wfk5dP5sI7Lsz4eG1tLatXr+app57isssu48EH\nH+SjH/0oZWVlPProo1RWVtLd3c0pp5zCBz/4wYwz3X72s5/h9XrZvn07mzdvTmj3+9///d/U1tYS\njUY599xz2bx5MzfffDO33347L7zwAvX19Qn72rBhA3fffTdvvvkmUkpOPvlkzjrrLGpqati1axcP\nPPAAv/zlL/noRz/KI488wrXXXlucX5aFxRFIx+YOWje0suL6Faa2TyvwWfrRRENRZEwSGhxTN/SS\nYDl4ExhjGhXPSCn5z//8T44//njOO+88Wlpa6OjoyLiPl19+WRfa448/nuOPP15/7KGHHmLlypWs\nWLGCrVu35mwktnbtWj70oQ9RXl5ORUUFH/7wh3nllVcAmD17NsuXLweytyS2sDhWWP/z9Tz5+SdN\nbx/oDVBWU5ZwX3lD5pbBYb+2XutEFPgjysFnc9ql5LLLLuNf/uVf2LhxI8PDw5x44oncc889dHV1\nsWHDBpxOJ7NmzUrbIjgXzc3NfP/73+ett96ipqaGT3ziEwXtR6FaDYPWbtiKaCyOdYJ9QSKBCNFw\nFLvTbmp7d7U74T5vgzdjBh8ejgv80MQTeMvBm6CiooJzzjmHT37yk/rgan9/P5MmTcLpdPLCCy+w\nb9++rPs488wzuf/++wHYsmULmzdvBrRWw+Xl5VRVVdHR0cFTTz2lP8fn8zE4OJiyrzPOOIM//elP\nDA8P4/f7efTRRznjjDOK9XYtLI4qgr2aYRrpHzG3fYaIJjQUIhJMXZ9BCfzIoLn9H06OKAc/nlx9\n9dV86EMf0qOaa665hksvvZRly5axatUqFi5cmPX5N954I9dffz2LFi1i0aJFnHjiiYC2OtOKFStY\nuHAh06dPT2g1fMMNN3DhhRcydepUXnjhBf3+lStX8olPfILVq1cD8OlPf5oVK1ZYcYyFRRrUxKVg\nf1CflZqJyEiESCCSdpAVtFr4qumJbbpDfs25WxHNEczll1+OsbVyfX09r7/+etpth4a0dRxnzZrF\nli1bAG25vuRyS8U999yT9v6bbrqJm266Sf/ZKOC33HILt9xyS8L2xtcD+NKXvpT5DVlYHCMogTfj\n4NU26cokQZvNmizwVkRjYWFhMU4YHbzZbdNFNJC+H40aZJ2IEY0l8BYWFkc1+Tj4jAJviGiS0R38\nBIxojgiBL+WqU8ci1u/T4lghEozoA6NjcfDGiCYZJfDh4TCxaGxMx1tsJrzAezweenp6LFEqElJK\nenp68HiOrIULLCwKQQk2jM3Be6o9CLtIG9GoQVYYjWsmChN+kHXatGkcPHiQrq6u8T6UowaPx8O0\nadPG+zAsLEpOgsAPFC7wwibw1qWfzaocPGg5vLvSnbLNeDHhBd7pdDJ79uzxPgwLC4sjEKPAjyWi\nAS2HD3SnThw0CvxEy+EnfERjYWFhUSiFRDQ2pw2n15nyWKZ+NMZYZqKVSloCb2FxlCFjkg13bkg7\n6/JYQwm8sAlTAh/oDST0gjeSqR9NckQzkbAE3sLiKKPlrRYe/6fH2f307vE+lHFHCXzltEpTEU26\nXvAKb0PuDN6KaCwsLEqKKuULHLIazQV6td9B1cwq0xFNNoEPHAqklEJaEY2FhcVhI9CjiZoxfz5W\nCfYFsbvtVDRWmB5kzSjw9V6QqSfO8HAYT432HCuisbCwKCnDPXEH32s5eCXY7ir3mB28PtkpKYcP\n+UNUNGrrMFsRjYWFRUlRDtNy8KOZurvKPXYHH29XkDybNTwcpnySJv4TLaIpaR28EGIvMAhEgYiU\nclUpX8/CwmI0ohnpm1hxwXigBNtT5SHsDxOLxLA5MvvanBENqf1owsNhKiZX4PQ6J1xEczgmOp0j\npew+DK9jYWHBqIO3IhpNsMtqy/TZpSMDI5TVlqXdVvWtySjwdZrAqxOoIuwP4/Q6cflcVkRjYWFR\nWqxB1lFUXbu7ShP4bDGNeiyTwKt9JLc8CA9rAu/2uSdcRFNqgZfAX4UQG4QQN6TbQAhxgxBivRBi\nvdVvxsJi7FgZ/ChqfVVPVbzKJctAq96moCa9wLsqXCBSTxLh4TDOcieuimPPwZ8upVwJrAE+L4Q4\nM3kDKeWdUspVUspVDQ0NJT4cC4ujH1VFc6wLvJQyoYoGcjj4LH1oAIQQuCvdKQ4+5A/pEc1Ey+BL\nKvBSypb4v53Ao8DqUr6ehYWFwcH3HtsCHwlEiIVj+iArZO8oqX5fmQQewFPlITQw6tJlTBIJRHCV\nu46tiEYIUS6E8Kn/A+cDW7I/y8LCYixEQ1FCgyHsbjvh4TDRUHS8D2ncMDpyPT83E9FkEXh3ZWK5\nper34/QeexFNI7BWCPF3YB3whJTy6RK+noXFMY9y7zVzagBzLXKPVpRgl9WU6Q5+LBENkBLRqMU+\nJmpEU7IySSnlHuCEUu3fwuJoZqBlgN49vcw8Y2Zez1MCXzu3lu7t3QT7gvoMzGMNVSZaVAdf5U6Y\nyaoajTnL42WSx0pEY2FhUTivfu9VHvrwQ3k/Tw2w1syNO/hjOIc3CrbD7cDutud08HaXHYcns+9N\ndvCq0Zge0QyFJtTyopbAW1hMQALdgYIu91MimmO4kibZkXuqPAkOft/L+7jr5Lt0160qbtL1glck\ntzzQHXy8Dh45sdZltQTewmICEuwLEh2JImP5uUE1yal2Xq2+n2OVZIFPbji259k9tKxrYe9Le/Xt\ns8UzkMbBxwXeVe7C5XMBE6sfjSXwFhYTECVO+a7KpDv4uZaDT+fgje67r7kP0IRebW9G4COBCNGw\nVp2UMMhaoQn8RBpozSrwQohThRA/EUJsFkJ0CSH2CyGeFEJ8XghRdbgO0sLiWKNQgR/uGcbmsFE5\nrRJI7EcTDUe56+S7jpmVnoJ9QZxeJ3aXHUh18L3NvQA0P9usb59L4JPr6VMiGiZWy+CMAi+EeAr4\nNPAMcCEwBVgM3AZ4gD8LIT54OA7SwuJYQznNcCC/PDdwKEBZXRlOrxOb05bg4AcODtCyroX9a/cX\n9VgnKsHeRMFOrmHv29uHsAs6t3Qy1D6kCXyGNgXGfUAagY9X0cDEimiylUl+PE0XyCFgY/z2v0KI\n+pIdmYXFMUzBEU1PgLLaMoQQeKo9CQI/2DoIwFDHUPEOdAKT7MiNg6yRkQiDrYMsuHQBOx/byZ7n\n9piLaJLKLY1VNMrBHxERjRJ3IcRNQoiabNtYWFgUj2g4qgtHJJB/Bq/a2pbVlCWUSSqBH+4cTvvc\no41kwTZWwPTv6wcJCz+0kLLaMpqfbTadwUOqg3eVu/QM/oiIaAw0Am8JIR4SQlwostUQWVhYjBlj\nTpx3RBN38ECqg285th28u8pNaDBELBrT8/eaOTXMPnc2u57cRXQkalrg9QgtLvCOMoce0RwRDl4h\npbwNmA/8CvgEsEsI8W0hxNwSH5uFxTGJUZQLGWQtq8sg8HEH7+/0p33u0UZypq4GSEODIfr2ahU0\n1bOrmXPeHP13ku8ga8gfwua0YXfaRwdZJ1AGb6pMUmpTs9rjtwhQAzwshPheCY/NwuKYJEHgC4ho\ncjl4f8cxJPBJDh40ce5r7sPmtOGb6mP2ubP1bQqJaFzlmnM/IiMaIcQXhBAbgO8BrwLLpJQ3AicC\nV5T4+CwsdKSU3H/x/ez4047xPpSSkq5boRnCgTCRQGTUwdd40mbw4eGwXr99tGLsBa8wNhzra+6j\nemY1NruNmjk1VM+q1rYpYJDV6XUCYHPYcHgcpiKajs0d7H1xb97vK1/MOPha4MNSyguklH+QUoYB\npJQx4JKSHp2FhYGenT3senIXzc83j/ehlBSj684ng1eTnNQgq3LwqjfKYOsgxEfQjnYXHxoKIaMy\nvYPvH6G3uVcXdSEEs8/TXHwugXd4HNgctgQHrwQeMN1w7KVvvMTj//R4fm+qAMxk8F8D6oQQN8cr\nalYaHtte0qOzsDCg6reHu4/uKpBCIxrVpsAY0URDUSLBCFJKBloGqDuuDjh6cvinv/g0L3/r5ZT7\n03WGTHDwe/uonl2tP7boQ4uwOW1Uzcg+f1Ot6mQcZHWWGwTeZE/4YH/wsPwNzEQ0XwXuBeqAeuBu\nIcRtpT4wC4tkDrx6ADjGBD6PiEY5eOMgq9pfaDBE2B9myoopwNFRSROLxNh410a91YCRdAKvHPxg\n6yDDXcMJAj//ovl8uefL+Kb6cr6uu8qtr+pkjGgAbVUnEwIfGgpp/YbCpV2QxUxEcy1wkpTya3E3\nfwrw8ZIelYVFGo5FB59PRKNaBesOPl5BEuwNMtAyAMDkFZOBo8PBt73dRtgfTiuo6RbQVg6+4+8d\nANTMTpzeo6pgcpHi4AuIaNQxl/qzbEbgW9FaEyjcQEtpDsfCIj1DHUMc2n0IYRMJCy4cjYz0j+ii\nUYiDN2bwoImdGmBtPKERODoy+P2vaCf8dOusZnPw7ZvaAfQMPl88VZ60VTSgRTRmBlnVSaDUn2Uz\nAt8PbBVC3COEuBttXdU+IcSPhBA/KunRWVjEUfHM9PdNZ7h7eEItqlBsgn1ByieVgxh7Bq/2pwS+\nZk4N7ip3UR38xl9t5K6T7yra/syiC3waQU23gLbD48DmtNGxWXPwxogmH4wtg0P+UEERjTrmieDg\nHwX+E3gBeBH4CvBnYEP8ZmExZoZ7hrnn7Hvo29eX9vH9a/fj8DiYc/4cIsGIPoNwPIiGo6z9f2t1\nx1xsVHmfw+PIO6JxeBy64CQIfLwG3jfVR/mk8qI6+O2PbKdlXcth/ZtIKfXILmtEYxB4IQSeKg+h\nQU2UyycVtpShu3K0K2XKIKvZiCa+jb+rtFdSOddklVLeK4RwAcfF79qpSiUtLIpF+9vt7HtpH61v\ntVI9M9VZHXj1AE0nN+mDYMPdwwmXxoeT9/76Hs/d+hyRYISzv3Z20fevBN5Z5sw7olHuHbReNKC1\nDB5sHcRd5cZV7qKisaJoDl5KycE3DgKaWKX725WC7h3dDHcPUzWjiv79/cSiMWz2Ub+qC3xVYtmj\nu9LNcPcw1bOqs67clA13lTtzmaSJiCYyEiEWjgETIKIRQpwN7AJ+AvwUeFcIcWZJj8rimEMJTrov\nR8gfom1jG9NPm463XsuXx3OgddeTuwDY8sCWkkRFRgefb0SjKmhgNHNWDl6dHMsnlRdURfP6D17n\nic89kXDfoV2H9DgkH7Hq39+f9+sbUfHMvIvmAantAYJ9QVwVLmyORIlTv5NC83cYHWSVUqZU0bh8\nWplkts+F8VgnQkTzv8D5UsqzpJRnAhcAPyjpUVkccyjBSXe53bKuhVgkxozTZ1DeoF1Wj5fASynZ\n9cQuHGUOenb26AN2xWSkf0QT+DJH3g5eDbACONwOHGUOPYOvbNIWASlvLC/Iwe9+ajcb7tyQMNP2\nwOsH9P+b3Wf7pnbumHkHB147kHvjDOx/ZT/lk8qZslIr+0z+3GTqDKkcfaH5O2gniVg4RtgfJhqK\nJkQ0bp8bGZNZ/25GgS91RGNG4J1Syp3qBynlu4Azy/YWFnmTzcEfePUACJh+6vg7+O7t3fTv6+es\n/zoLm8PGlge3FP01gn1B3FVuLaLJ18EbIhoYnc062Jro4AM9gbxrsAOHAsiopPm50ZnEB984ODo7\n1qTAd23rAtAHOwth3yv7mHHGjIw92DMJvO7gxyLw8X40Q+2aKUmOaCB7PxrjY+Me0QDrhRB3CSHO\njt9+Cawv6VFZHHMocUj3xdi/dj+Tlk7CU+0ZFfgSfTGklFld1btPvAvA8dcez9zz57L1wa15L4yd\njVg0xsjASEGDrGo1JyNlNSFaqyUAACAASURBVGUEerQMvmJqBaA5eMj/JKmiGOOSfy1vtDDtlGmA\neTfaf0CLZ3r39Ob1+sbn9+/rZ8YZM0ZXUUr63Iz0j+hibkQ5+OQa+HxQAj/Ypg1cJ0c0kL1lcEJE\nMwEE/kZgG3Bz/LYtfp+FRdFQi1AkfzFi0RgHXz/I9NOmA5ojFTZRMge/49Ed/GD6D/RJQ8nsfnI3\njSc0UjmtkqVXL6V/f39CTDFW1OBdvhGNlFJrFZzGwR/afYhYJDYa0cSrR/KNaVTV0O6ndyOlJOQP\n0bG5gznnzcHhcZje38ABbdJVoQKvqmdmnjEzs4PvD6YMsEJxMni1X+XgjYP9ZloGq2P1NnjHN4MX\nQtiBX0spb5dSfjh++4GUcuJ0tLc4KsiUwQd6AowMjDBpySQAhE1QVldWsi9G59ZOoiNRhtpSByGD\n/UH2r93P/IvmA7DgsgU4PA62PFC8mMZY3pdPRBP2h4mFYykO3lPtoWdnD4Ae0VQ0ak4+n1LJWDRG\nsD+Ib6qPgQMDdG/vpvWtVmRMMu3UaZRPKjftRtUAa19z+pLYXOx/ZT8un4vGExpHHXPSZKeMDr66\nCBm8imjaCoxo4uJfM7tmfDN4KWUUmBkvkywIIYRdCPG2EKJkrdP+tq2D5u4jf2besUymiMboaBXe\n+tI5H/WlNQ4kKvb8bQ+xSEwXeLfPzXGXHse2P2wjFokV5fVVfbVeRWPSwSe3KVB4arSGY0BCBg/5\nOfiR/hGQsORjSwDNxavyyKbVTXgbvIfNwR949QDT3zcdm9026pjTDLKmE/gTrjuBNf+3Ri8hLQS9\np02hEU38WKtnV2uT9ooY8SVjJqLZA7wqhPiqEOIWdcvjNb4AlKzrZP9wmFse2sR//vGdo3p249GM\nlDLjIKsSWuWaAMobyksu8MZl8xS7ntiFp8ajZ84AS69eir/Tz+b7Nhfl9ZWDd1e5cZSZz+DVLFZj\nFQ0knhh9TXGBj2fw+ZRKBnq1/Tee0EjD4gZ2P6UJfN1xdXjrvPk5+AP9CJsg2BfU95sPfXv7qFug\ndcXUF+AwfG6klBkjmtp5taz+/Oq8X9NIioNPqqKB7BGNeqx6djUyKhN6DxUbMwL/HvB4fFtf/FZh\nZudCiGnAxUDJ5jFXeZ3cumYhr+/p4eENB0v1MhYlJOwP61FEJgdvdGOldPDKlSU7eBmT7HpqF/Mu\nmJdQWz3/ovlMOXEKf/7En/nbv/9tzN0BjRFNPnXweifJNBm8omKy9rV1V7qxu+x5OXjj/udeOJd9\nL+9j/9r9+smuvMFc6WV4OEygJ6CXN+br4kNDIUYGRvSrkXSDrJGgNpEoV2/3QtEz+AIjGnUyUgO9\npczhzQj8NinlN4w3zDvyO4AvAxmvX4UQNwgh1gsh1nd1dZncbSJXnzSDVTNr+O8nt9M9ZA0PHGkY\nnWSyg1dO2ujgy+pLl8FncvAd73Tg7/DrE2sUDreDT679JKtuXMVr33uNe8++d0yzRBMEPo9BVpXl\nJk+/VyJXPqkcu9MOaFP2yxvza1egKmjKasqYv2Y+0VCUQE+AplOaAPBO0iKaXFfRAwe1eGbmWTOB\n/AVenYCVwDs8DoRdJHxu9M9MmoimGKiTSrpBVtNVNAK993wpc3gzAv8fJu9LQAhxCdAppczar0ZK\neaeUcpWUclVDQ4OJw0nFZhN858PL8I9E+Nbj2wraRyHImOSd+98peU/nox0liOWN5Zkz+KrUDL7Y\nkZyUUv/SJjt41culfkF9yvMcHgcX//RirnjwCg6+cZB1/7eu4GNIHmQ1G9Go36G3ISmiibfLVfGM\nonxSfpOdjA5+xukzdNeqO/hJ5VqPIH/241UDrErg8x1oVU3TfFO09yOESGnwpf526SKaYuBwO7C7\n7WkzeLMRjavCpZ+MS1kqmVHghRBrhBA/BppU58j47R60hbdzcRrwQSHEXuBB4P1CiN8V46DTMb/R\nx+fOnsefNrXy8ruFXQnkS8u6Fv54zR/Z+djO3BtbZEQJTe28WlMZvLfeW5LsMnAooA9IJjt4lRVn\nu+xf+rGl1MypoXtHd8HHYHy/+Qyy+jv9CLtIGTxUx5u8kEVFY0VeDl5//zVadDTrnFk4vU4al2nt\nh9UM41wnDVUDP2nJJLz13vwdfGuig4fR9gCKUjt40E4eSpiNAm932bE5bTknOrl9bv1kPF4RTSva\nhKYgo50jNwCPobUryIqU8j+klNOklLOAq4DnpZTXjvmIs/C5c+Yyu76cbz+5nVgJR6YVKlro2np4\nTihHK7rAz61N6eOhZ/BJAg/F/2IYSyOTHbzefrYmuyusO66OQ7sOFXwMwb4g7ko3NrsNR5mDWDhG\nLJq7Qsff6ae8oRxhS2yglUngC3bw8RPIBbdfwMce/Zg+HqFX5uSIG1QFja/JR/Xs6rwFXv2NjO/H\n7XMnGINSO3hI/DwaB1nTHU8yysGrz/G4RDRSyr9LKe8F5kkp7zXc/iilLKy+qcQcfH4vF9+/E/nU\ne/x1W/F7hCSjKhe6txfu2CxG67Gr51RrfTwMA4sj/SM4PA7sLrt+X6kEXl1yq9c1kq79bDpq59fS\ns6un4PhopG+0ftvh0Zq9mhloHe4cTtv+Vhf45Igm3o/G7HEGe4M4yhz6MdUdV8fc8+fqjys3asbB\nlzeW43A7qJlTk1HgpZQ8+x/P0rqhNeH+wdZBHGWOBHfu8rkS6uCNlUilwrhvZ1miwFfNrKJrS2bT\nFxoM4fK5cJY5cZY7xyeiMbBaCPE3IcS7Qog9QohmIUTqIohZkFK+KKW8pMBjzEn7pnZ+e/5v+d0F\nv8P/djuL9gzww+d2l9zFq9rjru2Wg8/GcPcwm3+XuYzQ3+nHXeXWhTthwGwgdcJKqRqOKXfoqnCl\njWic5U59oDITtfNrCfvDaSdKmcHYQ0UJh5mYxt/lTyvwalJT8szN8knlREPRtOWg6UhuRZyM2Tx5\n4MAAVdO1wcWaOTX07+tPe4Xi7/Tz6ndfZfNvEz83qqeOsdVvcgavzyU4DA7e4XGkXDXNvWAu+1/d\nn3YuBYw6eIiX/I6zwP8KuB04HTgJWBX/d0IQ7Avy69N+TduGNi74wQUsvXopDX0htrcN8Oz2wpsZ\nmUE5+J6dPaYuo48W3n3iXbY9Yn4we9M9m3j0448mOGQj/k5NnNJNWhnpH0m4HIbSO/j6hfVpIxoz\nk2PqjtPqs3t29RR0DEaBd5RpbtnMQKu/058ywApQOa2S69dez7KrlyXcn28tfLA3mF3gzWbw+/up\nnK61TKiZU0MsEtMra4yo2bfJDt/YNE3hrkwf0ZTUwcc/k8nxDMD8NfNTmrIZGRkc0QW+1O0KTC3Z\nJ6V8SkrZKaXsUbeSHVGeeKo9fOThj3DzezdzyhdPYfLyyUS6h5nndfHD53aVdPKTcvCRYIT+fWPr\nb30k8ep3X+XZf3/W9PZqYCyTq/V3+KlorEhbYjYyMJLixEqZwbsqXPim+tJGNLnyd4C6+ZrAF5rD\nB/sNAp9HRKNOkumYcdqMhIgLEmezDnUMce8592at/gkcCmQ9wTm9WtyQTeCllJqDnzHq4CF9qWT3\nzu60jw22DuoVNIq0g6zC/CLahaA+k8YBVsW0U6fhrnSz66ldaZ8bGgrpx+at9457meQLQoj/EUKc\nKoRYqW4lO6ICmL9mvv6lqF+klbF9fFotW1sHeG57Z8r22x7exhs/fGPMrxvoCeitUo+lmMbf6af3\nvd6U/h9b/7A1ZUEIGM3YM7nFbA4+2B9McfDOcid2t70kAl8xpQJ3lTutgzczcaZyeiV2l52ed8fu\n4M1GNOFAmNBgKK8l6FR0076pnXvPvpe9L+7ltf95LaMhCvRmj2iAnLNZR/pHCA2FEhw8pBd45eD7\nmvsSjmmobUjviqlw+VwpDt5d6U6JToqJq1IzI+lWFbM77cw5bw67n9qd9vcZGgrhrND+thMhojkZ\nLZb5NtriH/8LfL9kRzRGGhZrtfRLojCzzsv3ntlB2BCfRMNRnrr5KV76xktjdveBngCTlmpNsIwD\nrZFghN+c9xv2vbJvTPufqCiXltzP++273mbjLzem9NZQteXq33T7807yZnTwyQIvhNBq4Yv8xRhs\n09yhu8qdNoM3E9HY7DZq59UW7uANPVSUg88V0ajfQz4Cr7Z95ovPMHBwgJM+fxL9+/v1/jLJBA4F\ncl7B5JrNqkokVQZfOa0SYRfpHXy81DQ8HE5oYxEaCqVGNPEMXn2fR/pTr/qKTTYHDzBvzTwGWwbp\n3JJqMFWZJGgRzbg6eCnlOWlu7y/ZEY2R6lnVODwODu3s4baLF/NuxxB3vzqahe18bCdDbUMEe4Nj\nXnh4uGeY2nm1lDeWJzj4/a/up/m5Zn1ZsaOJaCiqVykYVzOSUtLylrbyUrKzzibwsUiM4Z7h7Bl8\nmiy1FO0KlIP3VHkYGRhJMACZFpBIR+382oIcvIxJfTUnGM3gczn4TLNYs+Gt9yJsAleFi2ufuZZz\nv31u1s6YuTJ49frZxEqVSCoHb3PYqJ5ZTd+e1MlOPTt79L+7OgGkq4EHzcHHIjGiI6NzGEqZv4Mh\ng88k8BdqM56NvfNB+xuH/KODrN56L5FA6RaRzzbR6Q7D/7+Q9Ng9JTmaImCz26hbUEfXti4+sLiR\n8xZN4o5nd9Hapw2Ibvj5Bv3STa0sUyhqDcyGRQ0JDn7Ps1qRUaZR9CMZ4xfYKPC97/XqteLqi6jI\nJvDD3cMgyZrBJzt4KI3AD7YN6hGNjMmE2YjBXnMZPGgCf+i9Q3l3CQwNhZAxmRrR5MjgM81izYbN\nYePSuy7lEy9/gunvm4670s38i+ez9aGtKQUDkRFNgHK9f9WuIBNqFqvK4EGLaXqbEx18NBSlt7mX\nuR/QyjBzCXxyT/hMjcaKSbZBVtCuTiYtm8TupxIFPjwcBjna0kAvLy2Ri8/m4I0La/9j0mPHl+BY\nikbD4gZdvL926RJiUvLNv2yjZ1cPe57dw4mfPREYm8CrBRa8dV7qF9XTta1Ld3zNz2pXDKXsEjde\nqDhA2AQdfx+NaFrWtej/N1bLRENRfZJMuismvU1BGgcvpUxbJgnF7yg5MjhC2B/GN8Wni4OKafRV\nlkwKfN1xdURHonokYRZ9gk7SIGuuiMb4O8yHFdevYPIJk/Wfl161FH+Hn30vJUaLeh+aXA4+nidn\nij77D/Rjc9j0pmegzX1IjmgOvXcIGZXMvSCDwKcZZIXRz02mVsHFRO0/k4MHLabZv3Z/gmFRpsFY\nJgmla1eQTeBFhv9PeBoWN9C/r5+QP8T0Wi83vX8+T29t50/feQWbw8aZt52Jp9pD59bUfMwsoaGQ\nvsBC/aJ6RvpHGGofInAooE/OGOk7+hqfKTFpWt1Exzsdeh/0BIE3OHijo0vn4I3ipD706gsR9oeR\nMZnWwRe74Ziq8FEOHkYF19ij3QyqkibfmCZ5MpXpiKZAgU9m/kXzcVW4UtaZTZ7Fmgm9tn4g/ed+\n4MAAvqk+bPZR2amZU8Nw13CCCKoB1sYTGvE1+fQIJ6eDj7/u4cjgc0U0oMU0sXAsoVxSvU9jFQ2U\nrl1BNoG3CSFqhBB1hv/XCiFqgeyzPcYZVUmjBmo+c8Yc5lWX0fz7rSy4bAG+KT4aFjfQva3wGajG\n/tsNi7SB3e7t3TS/0AxSc19Ho4NXYjLn/DlER6J6OVvLuhamnjQVSBR4JeqOMkdOgbc5tOn5yoml\nazSm8NZ7CfYGi7bQht6lMI2DV31YzC4SUTu/Fsi/VFIX+Kr8yiT9nX4cHod+giwUp9fJgssWsP2R\n7XpPHjC8/xwOPtds1oEDA3r+rlCVNMamY+p7W7+gPmG261DbEM5yp+7YFck94YP9pXfw+iBrhogG\ntPJUV4WL3c+MxjTJDn48I5oqtN4z64FKYCOj/Wh8WZ437qhKGpWLuxw2ro3ZcQ5HsMXbvdYvrh9T\nRGNcQUedULq2d7Hn2T24fC6mv2/6uAh8374+Hv7Yw1m72Y0F9eVV09TbN7UTDUdpf7udGafPwNvg\nTSvwjcsa0wq8Kp1UE2+MfTzSNRpTKOej3OVYyebgzfahUfim+nB6nXlPdkp28GbLJIe7hvE2eBNm\ndxbK0quWEjgU0MeRwPz7zzWbtX9/f0L+DqMCb7za6dnZo/0dKt0JAp9uFiskRjRSJg5UlwozDt7u\nstN4QqN+RaKOESZARCOlnCWlnCOlnJ3mNqckR1MkaufVYnPYEgQ88pfd+OvLeDCoCULD4gaGu4cL\nPnMqB19WV4Zvqg+Xz6U5+OeamXXWLLwN3nER+G0Pb2PrQ1tTengUC3+XH5vTRtPqJuxuO+2b2unc\n0kkkGKFpdRO+KT6GWkeFXBf45Y2M9I+kiJW/04/NYdO/kMZJK+kW+1AU+9I2m4M324dGIYTQBlrf\nLdDB5zmTNdskp3yZe/5cPDUetj08OlM502IiyWRbClDGJAMHUx1847JG3FVudj05Oimoe2e33pa5\nenY1Ay0DREYiaWexQuIgayQQIRaJjXsVjcI3xZcwwU938PGTkrvKjc1hGxcHn4IQ4uslOYoiY3fa\nqZ1fqwt86/pWWt44yPRrlrJ+fx/r9x7SF3EutBOkcvDeOs05NSxq4L2/vsehXYeYfd5sPNWecRH4\n1nWasPftLWxB41yoroV2p51JSyfRsalDz9+bVjfhm+pLdPBxh954fGPCzwn7m1SuuzJjX5F0i30o\nit2Jb6htCLvbjqfGk+Lg841oQMvh83XwyVl/PhFNsQTe7rJr4yuGAXSz719vV5Dmb+Lv8hMNRfUa\neOPrLfjgAnb8aQfRcBQpJd07uvUl+Wrm1ICE/n39GQXe6OAPRydJGDUd6SY6GamYUpFQdJAc0ehz\nOsYhg0/HB0tyFCWgYfFo6eK6H6/T6n2/ciY1Xic/f2mPHuMUGtMYHTxoub/KXOecN0cX+MO9TmzL\nW5rYmhV4KSWv/s+repaeC2PXwsnLJ9O+qZ2WdS2U1ZVRPbuaiqkVKRGNp9pD9cxq/edM+4PEWYnZ\nMvhiNxwbahuiYnIFQohUB59nRANQe1wtvXt681oMJrkLot1pR9iFqUHWYgk8QN2COrp3duuf3cAh\nbcZ2LlecLYNProE3svgjiwn2Bml+vpnh7mGCvcFEgUerrBls1cpYkzE6+MPRCx60k/C53zmXxR9Z\nnHW7iikVjPSP6FdhyYOsEO9HMw5VNOk4Yqpp6hfVc2j3IfoP9LPlwS2c8IkTqGmo4LpTZ/Hs9g46\nnJozLFTgk1exVzl8xeQKGhY34KnWVrM3u2BDMfB3+fXBqv695kr0Aj0Bnv3ys2z4RdaFt0ZfwyAm\nk5dPZrh7mF1P7KJpdRNCCHxTfQy1D+m11P52PxWTK/TSuGSBH+oY0vN3SHTwZjL4YkY0qvzOWe5E\n2MVoBp9nRAOag5dRmfNE2/xCM6//4HW6tnel7Vjp8GRfeFstWF5Mga9fUE/YH9ZXsQr2anXlxuqX\ndDjcDtyV7rRiNdASF/hpqQI/9wNzcflcbPvDNj2vrl+ofZ+UwLdvaicSiKR38IZ1UJMHqkuFEILT\nbz1dr5jKhPpMqZgm2cFDadsV5CvwJ5bkKEpAw+IGZEzyt3/7G9FQlNX/rK2kft2pM/E4bdz5cnNC\nvXwyUsqsK74HegLa4sXxL6OqpJlz3hxtGTF1mX8YY5rWt7R4xuFxmHbwavJJuinVm3+3mQOvH0i4\nz9812rVw8nKthtrf4dcraHxTfciY1D+wQ+1DCQKfXAufLE7pHHw6N6aunHIJ/Nt3v80r33kl6zYw\nOosV4svAVboTqmhsTlvOvNWImUqayEiER65+hL/e8ld+uvinrPvRuhRhcpY5s5qE0FCI6Eg0r0lO\nuVDiqq7qcrUKNpJpIRGV43vrUo/T4XFoMc2jO/TSZZXBV0yuwOFxcGCt9jlMJ/Cq+mpkcOSwdJLM\nB/WZUjFN8iArlLbhWE6BF0J8TwhRKYRwAn8TQnQJIUq6MlMxUBHM1t9vZe4Fc/UPTF2FmytPnMaf\nN7VSeVxt2gxeSsljn3qM26fenrFWXs1iVUxeMRmbw8Zxlx4HjLo9s/22i0HLWy0gtIEy0wJ/IL3A\nx6IxHv/s47z2vdcS7jcKssrVQcvfYfQLqHeQjDt09Ryjg0/nPhMGWeO/u3Tlfw63A5fPldX5BA4F\nePrmp3n+K8/nXEZvsC0x3/VUeRIiGk+1J68qFdU2OFv09c597+Dv8HP5by7n4p9fzLw181h2TWJb\nX0eZI2sGX6waeCMqHlFuOp9ZvN6G9LNZc8Vci69cTOBQgA0/34DdbadqppbVCyGomVPD/le1th/p\nBB7i1VcDI4elF3w+pHPwdpc9cQGbBq+pjqGFYMbBny+lHAAuAfYC84B/K8nRFJG64+r0QGn1TasT\nHrvu1FmEojEOlDvwd/pTXOCLX3+RTXdvIhaJ8eTnn0ybo6tZrIqq6VV8cf8X9UxOCfxhdfDrWmlY\n3MCkZZPoP9BvqkZcOfihtiE9dgLNeYb9YQ69N+pAQ/4QYX9YFxN3pZuaudoldNNJcYGfkiTwcQdv\nd9kpqy1LEPjQUIhIIJIgTsYyyZGBEVw+V8ZowDfFp8cI6Xjjh2/oX6i131mb8FigN0Dreu2KJxKM\nEOwNJuS7xo6SwT5zveCNeOu9VM2s4qWvv8Tbv3475TMkpeT1/32dxuMbOf7a41n1T6u4+rGr+cD3\nPpCwncNz+AXeN9WHq8JVsINPd9INHAogbCJjC9+5F8zFVeGifVO7VgWXNBlKCXdGga90Jw6ylrhM\n0izqylU5+JHBkZQ6/jU/WsO/HPiXkry+GYF3xP+9GPiDlPKIaHzuLHNSO7eWmrk1zF8zP+Gx4xp9\nrJ5dy8tB7UNjbBS28a6NvPzNl1n+yeVc9JOL2PfSPt65752U/Sc7eNAER7m8wy3wqtlX00lNVM+q\nRkalnntmQwk8JLr4to1tgNZjRomT+uIa44BpJ0+j7rg6XWCMDj7kDxEaDOkf8orJFQkCr4tTY6KD\nVzNY07UKNlI9q5q+femvVIL9Qd784ZssvHwhJ33uJDbft1mvp44EI/z2A7/ll6t/qTWfix+TcQp8\nioPPY4AVNOd53XPXMXnFZB771GPct+a+hIUtdj+9m65tXZz6pVOzXhnkimhKIfBCCOqOq6Nnh+bg\nzXbSVMeRNqLp1bpRZmrh6yxzctwl2tWviogU1XNGV6NKblOgUFd+h2uQ1Szeei82h0138OGhcMoV\naSnbGpsR+MeFEDvQ8vfnhBANaAtxT3g++OsP8pGHPpL2F/jxU2ayy6O9fRXTbP/jdh7/7OPMu3Ae\nl/z8ElZ+eiVNq5v467/+NUWokx18MoUKvIxJ3vjhG2z81ca8nte/r5/hrmGmrp6qL89mJqYZ2D+g\ni2g6gQ8Ph3UBTNe1cM2P13Ddc9fpP5c3loPQBF7l7UaBN2bw6cRJ70czFCI0EMp6qV01syrje3zr\nJ28x0j/CGbedwfu+9D5sdhtr/5/m4p+86UnaNrRRM6eGR65+RK/BzuTgA72Bghxh7dxa/vH5f2TN\n/61h/yv7uXPVnfqYxuvffx1fk4+lH1uadR+OsuyDrKUQeNBE1ujgPbXmI5rh7uGURmtmVsRadOUi\nYDQiUqiBVpfPlXG2rrryC/YH9S6ZEwFhE5Q3lusCb1zN6XBgpl3wrcD7gFVSyjDgBy4r9YEVg5ln\nzGTKyilpH7tgyWTKpviIeex0bu3k1f95lYeufIimk5q48qErtRI1m+Din13McPcwz3/1+YTnp3Pw\nRgoR+OGeYe6/5H6e+eIzvPLfuQcGjajySOXgwZzA9x/oZ+qqqVpvniSBVyfG3vc055tOTMpqyxIq\nI+xOO+WTyhlsHdRPDMqhJzv4Q7u1+MdYG23sKGnGwQ93Dae0Wg0NhXj99teZf9F8pp44Fd9UHys+\ntYJNd2/ixW+8yNt3vc0ZXzmD61+5nrK6Mp666Skgi4MvIKJRCJtg9edX85n1n8FV4eLec+7l+a8+\nT/PzzZx888kpKy0l4/A4sjp4vRd8Q3EFvm5BHf37+wkPh021ClaUN5QTi8RSPvdmroLmXzSfeWvm\nseDSBQn3K4HPFM9AooN3V7qLMqu3WPim+EYHWQ2rOR0OzAyyfgQISymjQojbgN8BU0t+ZCXG5bBx\n9ckz6Klxs/Gut3n2y8+y5CNLuO756xL+AFNWTmHVjatY/9P1euQRDWsNlYop8G0b27hz5Z00P9fM\n5BWTGTg4kFe72ZZ1LdrU6OMbtVpjYVLg49PHJy2dpK8EL6Wk/e12Zr9/NjAqxGbdoprspCY1KQdf\nPrmcofYhPfJpebMFV4VLLzEFEjpKZuokqdBPZEkxzfqfryfQE+DMr442RD3t308DCS99/SXmfGAO\nZ3/jbHxTfPzDE/+g9xPJmMEXENEk07CogU+/+WmmnTKNV771Cq4KFyfekLsozVnmzJnBu3wufVJU\nsahbUAcS2t5uIxaJmT7B6eWrPYk5fK4l/0B7r9c8eQ3TTpmWcH/N7NwCrzv4w9BJMl8qplQkDLJO\nKAcPfFVKOSiEOB04D20R7p+V9rAOD1evnkHvJC/RYIQzv3omVzxwhd7/w8iya5YhY1KPLVRFQLaI\nxuFxYHfZTQm8lJJH/uERZExy/drrWfGpFcTCMdMLIoM2wDp5+WTsLjsOtwPfVF/OWvhoOMpg6yCV\nMyppWNpAxzsdSKnVbgf7giy4fAHCLvSBVl3gc7hF31RterZy63pE01hBeDis1wIffOMgU0+amjCg\nZnTw6RbcNqIqLZLXw93y4Bamv296glBUz6zmpH8+ibrj6rji/iv012xc1shVf7qKlZ9ZmfC+3FVa\nVYaMyYIjmmS8dV4+/tePc/p/nM4Fd1xgap+56uCLXQOvUFVnB1/XVngy6+AzzU8ws+RfJqpnaydy\nsw5+olTQKIyzWUODoZRB1lJiRuDVVLyLgTullE8AEyPgGiNTq8uo+NRyXvr0UlZ/5YyMgx1qWT61\nRJ0+ySmLgxdCmG5XqNfYTgAAIABJREFU0L6pnZ6dPZz5X2fSdFKT3pBJzf7LRSwao3VDK1NXj15Y\nVc+qzungB1sHQWoLMDQu03rFDLYM6ieyaSdPo3pmNb27tYhmuGsYR5kjawc9iH+gVUQjRk8Ixlr4\ncCBMx987UtxaioPPEdFA4pWKjEm6t3frdflGLrj9Aj637XO6CClmv382l955acLf31PlQUa1Mk4Z\nlWN28Aq7y8653z6XlZ8yt6yxoyx7RFMqgVd1/GoJP9NlkqoJXE/iHJKxXAW5yl3MPX8uM8+amXmb\n+PyJw9FJMl98U3wMdw0TDUcnpINvEUL8AvgY8KQQwm3yeUcEN3xwEXvqPPzujczrp7p9Wle7zs1a\nRm1sFZwNswK/5YEt2Bw2Fl+hlViqTNrsghHd27sJ+8N6qSKYE3h9hZ3pVfpJrHNLJ20b27A5bExa\nOomauTUJDt7YNyYTvqk+hjqGGDg4oFcRAAmzWds2apf+yQKfksFn+bL6pviwOW0J73OgZYDwcDil\nEgO0k26u2ZgK9boq/ik0gx8rZurgSyHwrnIXVTOq8nbw6SagqUmDYzlJXvvMtZz4mcyRltvnJuwP\n67NuJxIq+vN3+NOWSZYSM5/2jwLPABdIKfuAWo6AOniznDizljPm13Pny3sYDmX+IjUe35iXgwdN\n4HNNdJIxqU/GUl8i1a/DWMKYDVXPbXSt1bOqc9bCG5dQa1iiTQzreKeD9o3tNCxpwOFxaAtIGzJ4\nM4N5vqk+kNC5uTNh9R6jwCtn2HRyU8JzlYMP9gUJ+8NZHbywCapmVCVENHov8UWpAp8PSiTUyWO8\n6qrNDLIWcxarkboFdfp8hrwzeIPAhwZDyKgs6UlSfU4GWgYmTA28Qp/s1D408Ry8lHIYeA+4QAjx\nz8AkKeVfS35kh5EvnDufHn+I+97IvEj2pOMn0fNuD+FA2LSDd1e5czr4g28cpH9/P0uvGi2XK6st\nw+l1mo5oOjZ34PA49NmTgKlaeCXwldMr8dZ5qZhSQec7nbRuaNWrj2rm1hDsDRI4FGC4a9iUW1RZ\nacfmjgSBV9U0Q+1DtLzRQvWsaioaE5tHqS+qEpZcbqx6ZuKVimowl87B54Pu4JXAFymiyRdnmTNj\nBi9jEn9XaRw8JJYrmnXwrgoXdpc9QeBVy49S/g6VKw70BCZcRKMcfP/+fqIj0QlXRfMF4D5gUvz2\nOyHETSae5xFCrBNC/F0IsVUI8Y2xH25pWDWrltPn1fOLl98jEErf/a/x+EZkTNK1rSsvB59L4N95\n4B2tF8dlo6VhQggqp1eaFvjOLZ00LG5IiB/UAGS2mGbgwABldWV6y9PGZY00P9fMcNcwk1dofWZq\n58V7qrx3yHQcoAQ+GoomCLi33ouwCYY6hjj45sGUeAZGv6hqUlA2Bw9QNasqoYqme0c3nmrPmEVP\nnVjU1cG4OvhAJO1s6kBvABmVJRN4NdAK5sVZb39rqKIxu6brWDCK5kQTeOXgVfvoCeXggU8BJ0sp\n/0tK+V/AKcBnTDxvBHi/lPIEYDlwoRDilMIPtbR84bz5dA+FuO/N9Fm86rvSsbmDQI/WfCrXHyqX\nwMciMbY9tI3jLjku5axeNaPKdETTuaVTz9AVZmrh+/f3J9SgNyxt0J2zcvC1czWB732vF3+nH++k\n3HGAsdqhfPKo+NjsNsonldO2oY2BAwM0ndKU8lyn14mwCb0FQa4va/XMaobahvQYo3tHN/UL68dc\nB53s4Mczg5cxmTZqK9UkJ4W6CrK77Hk1WiurKyPQPTrIWkg//Xwx5toTLYNXk/9U47mJJvCC0Uoa\n4v/P+e2RGqrOzxm/Hd7m6Hlw0qxaTptXx89fSu/ia+bU4PQ66djcoc9izSUiuQR+74t78Xf6WXLV\nkpTHKqdXmhpkHe4ZZqhtiEnLEgVeVeLkFHjDEmr6SULA5BM0B68mmbRtbCMaiprK4MsnlesVKcaI\nRv2sFiFO5+CF0GYhmnXw6kSmflfdO7rHnL9Dmgx+HCMaSL/oR6kmOSlUROOpya/RWvICFqqTZCl/\nhxPZwduddrz13lGBn2CDrHcDbwohvh5f0ekNtFr4nAgh7EKITUAn8Dcp5ZtptrlBCLFeCLG+q6vw\nNVKLwc3v11z8g2+lZvE2u42GJQ10bu7MOYtV4an2EAlGEgbJpNRy032v7OONH7yBy+di/kXzU55b\nNb2KofahhMWP06HaLCQ7eDO18P37+6mcMToLVe2jfkG97jKcXie+qT69msKMW7TZbQmzV42UN5YT\nDUWxu+x6u+FkXD6X7uBzuTFjFBXsDzLUNjTm/B1GRUJFNLlONKVCX9UpzUCrPlO4RA6+sqkSp9eZ\nd7SSLPB6RHOMOnjQYhq17uyEcvBSytuB64FD8dv1Uso7zOxcShmVUi4HpgGrhRApjTeklHdKKVdJ\nKVc1NDTkd/RF5uQ5dayeXcsvXtrDSCRVWFUlTaAnkHOAFQyzWftHXfy959zL9yd9n3vOvIddT+5i\n2TXL0k6uqppRBZKcDcM63tEqe5IFHrKXSqrWqkYH37C4AQQp7R1q5tbolTpmxUTFNMmDqErwJ6+Y\njMOdfval2+fWJ3mZdfB9e/tGK2iKIPCuChfCJggPh7V1M02WVxabbOuytr3dhs1p02vWi42wCeoW\n1Jn6rBtJzuD1iOYYzeAhPps1fkI+nIOsWec3CyHswFYp5UIgv+5XBqSUfUKIF4ALgS2F7udwcNP7\n5/HxX63jD+sPcu0piRMrGo9v5O1fvU3Xti5mnDEj576M7QoqGiuIBCPse2kfCy9fyMobVlK/sF4X\nqGRUqeTAgQF9qnY6Ord04q5y42tKneVXPauaA68dSPOs0UgjoQ9MuYsLbr+A6adNT9i2dl4t+1/R\nrmrMluT5pvpo29CWNqKB9PGMfhw+lx7m5fqyVjZVIuyC/n39el+XYgi8WvRjLH1oikG2dVlb3mhh\n8vLJaQ1CsVjz4zV5j2eU1ZUR6AkgYxJhEwR7g9gctpwT5MbCkeDgFRPGwUspo8BOIURuNUtCCNEg\nhKiO/78M+ACwo6CjPIycPq+e5dOr+dmL7xGOJg5sqYHW4e5h0xENjPaj6W3WZoQuumIR89fMp2Z2\nTcYvT7rJToHeAO1/b0/YrmtLF43LGtPup2pWVcZaeGMNvJFTvnhKwoQpQO/5DgU4+AIEPsGN5XDw\nNoeNymmVuoO3OW36uMFYUSeX8ayr1jP4pIgmFonR8lZL1t9jMZhx2gymv2967g0NeOu9Wrvn+Oc+\ncCiQd46fL8bPzESrg4fEYoOJlsHXAFuFEM8JIR5TNxPPmwK8IITYDLyFlsE/PpaDPRwIIbj53Hm0\n9AV49O2WhMeMA5l5RTRK4ONdGY2CmYl0k52ev+15fnXKr/T9SSm1Esml6aOtbLXwmQQ+HapUEswP\n6E05cQpVM6pSLssnL5+Mq8LFzDOzTzsHEHZhqnqjemY1/fv66dnRQ938On3m7FhRTnC8Blghc0TT\nubWTsD9ccoEvhOTJTmZaBY8VR5kDYddOIBMxohkvB2+mBd1XC9mxlHIzsKKQ54435yyYxJKplfz0\nhd18eEUTjnj+6q3z4mvSVhHKx8Gr2axqyr8qPcyGq9xFWW1ZQi1883PNRIIRdvx5B8v/cTmDrYME\n+4Jp83dIzKerZyZGQQMHBhB2kXaV+mTU8bor3aa7Fp74mRPTTi2fdfYsbu2/NesiB8qNmW37Wj2r\nmubnmxnuHtaXaiwGSigmREST5OAzzQSeCBg7StZRN+Y2BWYQQlstKtg38VoVQFKn0okw0UkIMU8I\ncZqU8iXjDa1M8uBhO8JxQAjBF86dz96eYR58KzHDVjGNGQefvPB273u9uCpcpnNs42SnofYhfY3M\nrb/fCkDnO1pvnEwCr2Ie40pCiv79/VQ2VZoaPFRXHMWaEp9rBRvl4M1+UatmVTHQMsCh3YeoW5h9\nlft8UK/vrh4/R5ipTLLljRa89d6ixVHFRH03Ehx8CQdYFS6fS7vqK2HWXyhGB5/PnIKxku3bfQeQ\nroSjP/7YUc0HFjeyelYtP/jbuwwGRy+PVUxTUAa/p5eaOZlz92SMk532vaxNwJp19iz2/G0PgUMB\nfYGOTAKvBl4zCbyZeAY0B1tWW1aycrxklMCbLU2snlkNUsulizHAqphIDj45ojn4hjYTeCItbKFI\njmjM9IIvBm6fe8It9qFQDt5Z7izpEn3JZBP4RillymKk8ftmleyIJghCCG67ZBE9/hA/e/E9/X7d\nwdfndrNOrxObw5bg4M3k7wrjZKe9L+7FVeHi3O+eSywSY/uj2+nc0knFlIqMVxNunxt3lTutwA8c\nGDAt8AAzzpihty8oNXpEYzJLNVYilULgx3PQTmXwxogm0Buge0d32pnAE4EUgT8MEQ1oxmAixjMw\n6uAPZzwD2TP49PV7GuNnaQ4jx0+r5vLlU/nV2mauOWUmTdVlLL5iMcG+oKnBLWNPeBmT9Db3Mv+S\n1ElNmaiaXkWwN0hoKMS+l/Yx/bTpNK1uomZuDVt/v5XAoUBG966onFbJ4MHBhPtkTNJ/oJ/FH1ls\n+liu+tNVprcdK/k6eDXZCYor8BNhkDVdRNOyThv8n4gDrKC5VLvbrpdKBvvGviKWGTxVHqIj2ScG\njhdOrxN3pfuwrxWbzcGvF0Kk9JwRQnwa2FC6Q5pY/NuFCwH4n6e1Ck+Hx8Hqz682PfFFCfxAy/9v\n777j46quRY//1hTNqBerWM2WjFxwxcZgsMGAgQRyCWDgUgOEkJBPCuWSRu59eSnv8u5Lbsp9EAg1\nlITQ4YZAgATboRlsbHAv2JZtWbZ6H5Wp+/4xM0LGki0JSWdmtL6fz3w0c6acvXXspT3r7LN2O0Fv\ncFAnWKOiI+yaD2to2NZA2ZlliAizrpjF3pV7+61B82kZJRlHjOA9tR5C/tCQRvBjKTrKGXQOvjQT\nJJySGskRUiylaPqO4KvfrwbhiOmssUJESJkQvprV2+4FM7oXOUUt/d9LOefn54z6foYrrTBtTKdI\nwtFH8LcBL4rINXwS0BcSXs1p+Wg3LFYUZyVz42nl3PuPPVy+sJTFFUMbIUYD/FCmSEZFp0puemIT\nQO+KNrOvmM07//cdgsHgETVojviMkgzqNtYdti06Hz8WT9DBJyP4pIzB/WewJ9lJL0of0dE79BnB\nx0CKpm8O/uD7B8mflW9Z+YTBiJYriNahGYs/kpOWDPlynTGVd3zekNZZHgkDBnhjTB2wWETOAqIl\nBl4xxqwck5bFkG8vq+D1rbXc9vQGXrttKTmpg/8r3BvgK8NBdUgj+MgsmG3PbMOZ4qRoYXhBj/w5\n+UyYPoGmnU2DGsF76jy99V+A3rbEaoAf6gge4HO//Fxv/ZsRa0c0B2/lPPhPXclqQobqNdUcf+nx\nlrVpMHoD/BjUgo8XFz928Zjv82jTJNMAjDGrjDF3R24r+3tNoktJcnDXVfNp7fLz/ec29lubeyDR\nAN+8pxmxy5DSIunF6SDhWTilS0qxO8MBWkSYd908nKnOY877zijJAEPvor8QCfByeO46lgw1Bw8w\n+8rZlJ9VPqLtKD+rnHnXz+utrGkFm92GzWnrTdE07Wqip2Vw54CsFC1XMBaFxuKFK8M15t+6jpZI\n/rOI/EpElopI79BIRKaIyI0i8jrh2jLjwqyiTO44fwZvbK/n8fcGXr/101xZrt4UTdbkrCFdZWl3\n2nvPvn96weElP1jCzR/f3LtYx0AySiI1bfrk4VsrW8koyRiw2JfV+l7oZKW0iWlc/OjFYzpvuT99\nV3WKVvUsWRTbAV5H8LFhwGhjjDkbWAF8nXCpgnYRaQL+CEwErjfGPDc2zYwNNywpY9mMfO7863Y+\nrus49hv4ZF3WoU6RjIqO+MvOKDtsu81uO2xhjYH0F+Cb9zTHbHoGwnVsouvBqsPXZa16twp3lntE\nr9gdDSm5KeFlHiNTJcfiJKs60rGKjf3VGHONMabMGJNhjJlgjFlsjLnTGFN7tPcmIhHhF5fNJTXJ\nzg+e30RwECdM3Jlu/F1+Gnc2DjvAO5Idhy2oPRT9BfjoBVexKiU3he/Wf5fjPn+c1U2JCY5kR28O\nvurtKkqXlI7pxTLDES041rrX2hWxxjtrilzHsdw0Fz/+4iw+qmrlsdX7jvn66AwMX4dvSCdYoxZ/\nfzEXP3rxsNMprkwXzlRnb4D3d/nx1HhiOsBDOE0Ti1ckWsGZ7CTQHaCzvpOmnU2DKlVtteiV3s27\nmrEn2XtnA6mxpQF+GC46oYizpufxn6/v5EBz11Ff23eK3XBG8EUnFjHr8iOX9BssESGzNLP3Yqfo\nAiCxHuDVJ6Ipmqp3wjX5J58+cCXOWBG9mrVpV9OolwpWA9MAPwwiwp3L52AT+NcXNx91Vk3fAD+c\nEfxI6HuxU6xPkVRHciQ78Hf7qXqnCrvLTuGJhcd+k8WiAb55d7Pm3y10zAAvIseJiCty/0wRuSW6\nkMd4VpSVzB3nz+DtXY3cvXL3gK/rG+Czyq35tWmAj28OdzgHX/V2FSWLSmJ29lNf0QAf9AY1/26h\nwYzgnweCIlIBPACUAn8a1VbFiWsWTebSBSX8+u8f8+i7e/t9TTTAp+anjnmhoaj0knQ6ajoIBUK0\nVA6tZLGynjPZSVdjFzUf1cRF/h0OL6etUyStM5ihQMgYExCR5cDdxpi7ReSj0W5YPLDZhJ9fOoeO\nHj8/+cs2MpKdXLLg8PnJ0QA/nPz7SMkoycAEDZ46z5BLFivrOZIdvQuKx0uAjxYc0xG8tQYzgveL\nyFXA9UB0yb3Yq6hvEYfdxl1XzWfxcRP43nObWL2n8bDnowHeqvw7HD5VMtanSKojRcsViE0oPXVo\n66NaRUR60zQ6grfOYAL8DcCpwJ3GmL0iUg78YXSbFV/cTjsPXreQkuxk/teLW/AGPilZ6kx1kl6c\nbunSar0B/kA4wGdNGfenUOJKdIphwbwCy6/uHYpogNeTrNY5ZoA3xmwzxtxijHlSRLKBdGPMz8eg\nbXEl1eXgpxfOorKxkwffquzdLiLcWnkrJ33zJMvaFg3wh9YfItAd0BF8nImO4OMlPRMVzcPrCN46\ng5lF8w8RyRCRHOBD4EER+fXoNy3+nDk9ny/MmcjdK3cfNj/enmS39MrD5JxkHG4H+98M19DRAB9f\noot+xMP89756R/Cag7fMYFI0mcaYduAS4HFjzCIgdqvqW+xHF8zEbhN+/NLWIVWdHE0iQkZJBoc+\nOARogI830UWkJ50WXyP45NxwYNcRvHUGE+AdIlIIXM4nJ1nVAAozk/mXc6axckc9P/rzFnbXe6xu\nEhBO04QCIZDIItUqbsz/ynwufepS0ibGV3VuzcFbbzDTJH8GvA68a4z5QESmALtGt1nx7ctLythZ\n18HTHxzgj+9XccqUHH5y4SxmTMywrE3RPHxGcUZvTlfFh8zSTDKviM3a/UcTzcFrisY6gznJ+qwx\nZq4x5huRx5XGmEtHv2nxy2m38ct/nsfqO87m++dNZ3e9h5seX4/HGzj2m0dJekm4tLCmZ9RYKV9W\nTsV5FZZdwa0Gd5K1REReFJH6yO15EYnt1QZiRF66i2+eWcG915xIdUsXP31pq2VtiY7gNcCrsZI/\nO59rXr2m9ySxGnuDycE/ArwEFEVuf4lsOyoRKRWRVSKyTUS2isitn62p8evk8hy+eWYFz66v5tXN\nNZa0IRrgdQ68UuPHYAJ8njHmEWNMIHJ7FBjMcjIB4DvGmJnAKcC3RGTmZ2hrXLv1nKnMK8nkjhc2\nU9vWM+b7zy4Pj9wnTJsw5vtWSlljMAG+SUS+JCL2yO1LQNOx3mSMqTHGfBi53wFsB6y7nNNiTruN\n/7pyPv5giK89vo72Hv+Y7r9gbgFXv3I1x19y/JjuVyllncEE+K8QniJZC9QAlwFfHspORKQMmA+s\n6ee5m0RknYisa2hoGMrHxp3y3FTuuXoBO2rbueGRD+gc45OuU78wFbvTPqb7VEpZZzCzaPYbYy40\nxuQZY/KNMRcDg55FIyJphEsO3xa5YOrTn/+AMWahMWZhXl5sLyQ8Es6akc9dV87no6oWvvrYOnr8\nwWO/SSmlhmG4KzrdPpgXiYiTcHB/whjzwjD3lXDOn1PIry6fx/t7m/jyI2tp6fRZ3SSlVAIaboA/\nZmEVCRccfxjYbozR2jWfsnx+Cb+5/AQ+3N/KRfe8y8d1HVY3SSmVYIYb4AdTZGUJcC2wTEQ2RG5f\nGOb+EtLF84t56uun0O0Pcsm9q1m5o87qJimlEogMVBBLRDroP5ALkGyMGfHr3RcuXGjWrVs30h8b\n82rauvna4+vYXtPBf14294hVoZRSaiAist4Ys7C/5wYcwRtj0o0xGf3c0kcjuI9nhZnJPHXTqSwq\nz+H2Zzby+3f6X99VKaWGYrgpGjXC0lwOfv/lk/j8rAJ+9vI2fvn6TkKh2Cg3rJSKTxrgY4jbaeee\nqxdw5Uml/HbVbr7xhLUFypRS8U0DfIxx2G38xyVz+NEFM/n7tjouvXc1+xo7rW6WUioOaS49BokI\nN55WzvSCdL71pw8561f/YG5xJkun5XHe7InMKoq/2uBKqbGnI/gYdtrUXF655TRuO3saDruNe1bt\n5ot3v8ODb1XGzHKASqnYNeA0SSuM12mSg9Xa5eOHL2zm1S21LJ9fzH9cMge31pZRalwb1jRJFXuy\nUpK495oFfOfcabz40UGuuP89Gj1eq5ullIpRGuDjjIhw89lTefC6heys6+Cf73uP6pYuq5ullIpB\nGuDj1LkzC3jiq4to8ni59HertZaNUuoImoOPcztq27nu4bW0dvuZnJNCUVYyFflp3LysgqyUJKub\np5QaZZqDT2AzJmbwwjcXc90pk5mSl0pzp4/H39vHpb9bzYFmTd0oNZ7pCD4Brals4qY/rMdpFx6+\n/iTmlepC20olKh3BjzOLpkzg+W8sxu20c/n973H7MxtYsb0Ob0BXj1JqPNErWRNURX4aL35zCb94\nbQevb63lhQ8Pku52cP7siSyfX8Ki8hxstmOu26KUimOaohkHfIEQ7+5p5OWNNby2pYZOX5DirGQu\nO7GEK04qpSgr2eomKqWG6WgpGg3w40y3L8jfttXy3Ppq3tndiABnTc/n9s9N0xo3SsUhDfCqXwea\nu3j6gwM8ubaKjp4A3z9vOl9ZUq6pG6XiiJ5kVf0qzUnhu5+fzhu3n8EZ0/P491e2c8OjH1DT1m11\n05RSI0ADvCI7NYkHrj2R/3PRLN6vbGLpL1bxvWc3skuvjlUqruksGgWEa9xce2oZZ07P56G3K3l6\n3QGeXV9NbloS4XXWweWwkeqyk+ZyMLcki28vqyA3zWVtw5VSA9IcvOpXc6ePJ9dWcbA1nK4xJjwb\nx+P1094dYO2+ZpKddr51VgU3LCnTssVKWURPsqoRt7vew/97dTtvbK9nUk4Kdy6fzelT86xullLj\njp5kVSOuIj+Nh64/iSe+ugiHTbj24bXc/swGmjt9VjdNKRWhI3j1mfX4g/x25W7ue3MPTruNc2YW\ncMHcQs6YlqepG6VGmaZo1Jj4uK6DR1fv49XNNbR0+QEQAafNRnaqk8sXlvKlUyZTkOG2uKVKJQ5L\nAryI/B64AKg3xswezHs0wCcGfzDE6j1NbDzQij8Ywh807K7vYMWOeuwiLJ2WRyBkaPJ48XgDFGcl\nMyUvlWkF6SyfX0y622l1F5SKG1YF+KWAB3hcA7wC2N/UyWOr97NqZz0Zbge5aS5SXA4ONHdR2eCh\nvSdAYaabO5fPZtmMAqubq1RcsCxFIyJlwMsa4NWxGGP46EArP3x+MzvrOrhwXhFfOa2cOcWZ2COl\nE3r8QXbUdnBcXqqO8pWKiOkALyI3ATcBTJo06cT9+/ePWntU7PMFQtz7j93cs2o3/qAhO8XJKVMm\nUNvew5aDbfiDhvx0Fz+7aBbnzS60urlKWS6mA3xfOoJXUU0eL+/sbuTNjxtYU9lMYaabhWU5TJ+Y\nxkNv72XroXY+N7OAW86eyvGFGb2jfIC2bj8pSXacdp0FrBKfBniVUALBEA+/s5ffvPExPf4QGW4H\nJ5fnAML2mnYOtnZTnJXM/deeyOxiLYGsEpsGeJWQ6jt6eHd3I2sqm1m7rxkBZhVlMjU/jSfXVtHU\n6ePO5XO4cF4RK3fU8dz6gwDcuXy2TtVUCcOqWTRPAmcCuUAd8GNjzMNHe48GeDVSGj1ebv7TR7xX\n2US620FHT4D8dBceb4BUl4P7vnQiJ07OBsAbCNLQ4SU/3U2SQ9M6Kr7ohU5qXAoEQ9yzag97Gz1c\ndEIxp0/NZU9DJ197fB01bd1cdfIkdtV5+LCqBW8ghAjkpbkom5DK4ooJnDk9/7BZPErFIg3wSvXR\n2uXjlqc28PauBmYWZnByeQ5T89Op7+jhUGs3O+s8bKpuxRjISnEypziTuSWZTJ+YQTAUwtMToMMb\noNMboNMbpMsXID/dTXluKuV5qcwuytRvAmrMHC3Aaz14Ne5kpSTx+FdOxhsI4nL0XyunpdPHW7sa\nWL27ic0H27j/zUoCocMHQ3abkJpkx+2009TpIxh5Pt3t4NzjCzhv9kRyUpNo6PDS6PEysyizNy2k\n1FjQEbxSg9DjD7K/qQuXw0aa20Gay4HLYUMknL7xBUIcaOliV10HK7bX87dtdbR1+4/4nKXT8rj9\n3GnMLc7kYGs3lY2dZCU7mVuS2ftZSg2FpmiUGmP+YIgP9jbjD4UvzMpKcfLShkPc9+YeWrr8JDls\n+AKh3tfPmJjO1YsmMbs4ky0H29h4oI2eQJBl0/M5+/h8slKSLOyNimUa4JWKER5vgCfe30+jx8uU\nvDTKc1OpbOjkT2v3s+Vge+/rctOSsNuEunYvdpuw+LgJ/NOcQs6bPfGYwb7HH6St209emgubniBO\neBrglYoDm6vbONTWzZziTAoz3RgDmw628frWWl7dXMO+pi4cNuGkshyKspLJTUsi1eWgtr2Hgy3d\nHGrtpq69h/bDqELrAAAJ6UlEQVSeABA+F3BCaRYzizJo9vjYVe9hf1MnpTkpLCrPYVH5BKYVpFOQ\n6er3XIQ/GApfT+Dxce7MAmYVZWgaKQZpgFcqzhlj2Hqonb9sOsR7e5po7PDS6PHhC4bISU2iOCuZ\noiw3EzPc5Ge4SXc72FHbwYaqVnbWdZCTmkRFXhqTJ6RQ2dDJhgOt+IKfpIhy01wsnJzNxfOLOWtG\nHhuqWvnRn7fwcZ0HkfCavMVZyZwxPY85xZnMKsqgJDsFXyCELxAi3e0gO1XTSFbQAK9UAjLG4AuG\nBpwJFBUMmSPm8vf4g2w80Mr+5i5qWnuobuli1c4GGj1e0lyO3jr9P7lwFgsmZbFiez2vb61l7d5m\nOryBI/bhtAvXLJrMt5dVkJvmGrAtXb4Aa/Y2s/VgGwsmZXNyeQ4OrRn0mWiAV0odUyAY4t09Tbyy\n6RBFWcl8felxJCcd/scjFDIcaOli66F26tp7SHLYcDnsrN/fzDPrqnE7bFyyoAS7TejyBejyBQmG\nDMGQobXbz4aqw785ZKc4WTajgMJMN26nDbfTTn6Gm+IsNxMzk+no8VPd3E11Sxd1HV4aO7w0eLwk\nO+2UZCdTmpPCnOJMTijN6k0fVTZ4uO/NPbR2+fnq6VMidYpGXqc3QLLTbvl5Dg3wSqlRt6fBwy9f\n38mK7fW4nDZSkxwkJ9lx2AS7TXA77ZxUls3pU8NpnjV7m3l9ay1vftxAa5eP0DFCkdMuTEh1kZue\nRJcvSHVLd+9MpKJMN5+fPZFGj49XNh0iyRHef1Onj1OnTOC6UyeTm+4i3e0gKzmJvHRX77eaTm+A\nzQfb2NvYSUqSnQy3k5zUJGYVZRzx7cLjDfDallpe+LCa9yqbmFmYwR3nz+D0qXmj8jsdDA3wSqmY\nZozBHzR0+4LUd/RQ3dpNbVsP6W4HJdkplGQnMyE16bCTvKGQocHjZfWeRl7ZVMtbuxpw2oRrTy3j\nxtPKSXM5eGLNfu5/q5KGDu9h+7PbhIkZblxOG/saO/v945Kb5uKCuYWcc3wBO+s6IqWrm/AGQkzK\nSeHcmQW8tqWWg63dnFaRy9JpubgcdtxOG/kZbiry0ijOSqbTF2DjgTY+rGpBgJPKczihNIsku419\nTZ1srG6lyePjq6dPGdbvTgO8UirhdXoDiEBK0uEX6EdXAuvo8dPRE6Cly0dNa7gshccbYGZRBvNK\nsphakIY3EKK92091Szd/3VzDih31vd8SpuSlsnRqHl+cV8iCSdmICN5AkD++X8U9q3bT3Ok7ok0u\nhw1fMIQx4QXoIXzCOpzastERmfGUm5bE2n89Z1jpHg3wSik1DO09fj7Y28y0gnRKc1IGfF0wZOj2\nB/H6g3T7g9S09bCn3sOeBg8pSQ5OnJzNCZOyCIUM6/a1sGZvE12+IPNKsphbmklFXtqwTzZrgFdK\nqQR1tACv85OUUipBaYBXSqkEpQFeKaUSlAZ4pZRKUBrglVIqQWmAV0qpBKUBXimlEpQGeKWUSlAx\ndaGTiDQA+4f59lygcQSbEw/GY59hfPZ7PPYZxme/h9rnycaYfqudxVSA/yxEZN1AV3MlqvHYZxif\n/R6PfYbx2e+R7LOmaJRSKkFpgFdKqQSVSAH+AasbYIHx2GcYn/0ej32G8dnvEetzwuTglVJKHS6R\nRvBKKaX60ACvlFIJKu4DvIicJyI7RWS3iNxhdXtGi4iUisgqEdkmIltF5NbI9hwR+buI7Ir8zLa6\nrSNNROwi8pGIvBx5XC4iayLH/GkRSbK6jSNNRLJE5DkR2SEi20Xk1EQ/1iLyL5F/21tE5EkRcSfi\nsRaR34tIvYhs6bOt32MrYXdF+r9JRBYMZV9xHeBFxA7cA5wPzASuEpGZ1rZq1ASA7xhjZgKnAN+K\n9PUOYIUxZiqwIvI40dwKbO/z+OfAb4wxFUALcKMlrRpd/x94zRgzA5hHuP8Je6xFpBi4BVhojJkN\n2IErScxj/Shw3qe2DXRszwemRm43Ab8byo7iOsADJwO7jTGVxhgf8BRwkcVtGhXGmBpjzIeR+x2E\n/8MXE+7vY5GXPQZcbE0LR4eIlAD/BDwUeSzAMuC5yEsSsc+ZwFLgYQBjjM8Y00qCH2vAASSLiANI\nAWpIwGNtjHkLaP7U5oGO7UXA4ybsfSBLRAoHu694D/DFwIE+j6sj2xKaiJQB84E1QIExpibyVC1Q\nYFGzRst/Ad8HQpHHE4BWY0wg8jgRj3k50AA8EklNPSQiqSTwsTbGHAR+CVQRDuxtwHoS/1hHDXRs\nP1OMi/cAP+6ISBrwPHCbMaa973MmPOc1Yea9isgFQL0xZr3VbRljDmAB8DtjzHygk0+lYxLwWGcT\nHq2WA0VAKkemMcaFkTy28R7gDwKlfR6XRLYlJBFxEg7uTxhjXohsrot+ZYv8rLeqfaNgCXChiOwj\nnH5bRjg3nRX5Gg+JecyrgWpjzJrI4+cIB/xEPtbnAHuNMQ3GGD/wAuHjn+jHOmqgY/uZYly8B/gP\ngKmRM+1JhE/KvGRxm0ZFJPf8MLDdGPPrPk+9BFwfuX898OexbttoMcb80BhTYowpI3xsVxpjrgFW\nAZdFXpZQfQYwxtQCB0RkemTT2cA2EvhYE07NnCIiKZF/69E+J/Sx7mOgY/sScF1kNs0pQFufVM6x\nGWPi+gZ8AfgY2AP8m9XtGcV+nkb4a9smYEPk9gXCOekVwC7gDSDH6raOUv/PBF6O3J8CrAV2A88C\nLqvbNwr9PQFYFzne/w1kJ/qxBn4K7AC2AH8AXIl4rIEnCZ9n8BP+tnbjQMcWEMIzBfcAmwnPMhr0\nvrRUgVJKJah4T9EopZQagAZ4pZRKUBrglVIqQWmAV0qpBKUBXimlEpQGeDWuiEhQRDb0uY1YwS4R\nKetbIVApqzmO/RKlEkq3MeYEqxuh1FjQEbxSgIjsE5FfiMhmEVkrIhWR7WUisjJSi3uFiEyKbC8Q\nkRdFZGPktjjyUXYReTBS1/xvIpJsWafUuKcBXo03yZ9K0VzR57k2Y8wc4LeEq1gC3A08ZoyZCzwB\n3BXZfhfwpjFmHuE6MVsj26cC9xhjZgGtwKWj3B+lBqRXsqpxRUQ8xpi0frbvA5YZYyojRd1qjTET\nRKQRKDTG+CPba4wxuSLSAJQYY7x9PqMM+LsJL9qAiPwAcBpj/n30e6bUkXQEr9QnzAD3h8Lb534Q\nPc+lLKQBXqlPXNHn53uR+6sJV7IEuAZ4O3J/BfAN6F0zNnOsGqnUYOnoQo03ySKyoc/j14wx0amS\n2SKyifAo/KrItpsJr6z0PcKrLN0Q2X4r8ICI3Eh4pP4NwhUClYoZmoNXit4c/EJjTKPVbVFqpGiK\nRimlEpSO4JVSKkHpCF4ppRKUBnillEpQGuCVUipBaYBXSqkEpQFeKaUS1P8A/jxt8dyeTooAAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaR7MgIQsimR",
        "colab_type": "code",
        "outputId": "a742343c-0e68-4e59-9064-fcd9290c9c23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(history.history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'val_loss': [3.8122141361236572, 3.860191822052002, 3.3222291469573975, 3.5263752937316895, 3.7408523559570312, 3.2110774517059326, 3.2849197387695312, 3.3840787410736084, 3.2399888038635254, 2.7523226737976074, 3.9328255653381348, 3.3875255584716797, 3.7173752784729004, 2.3088693618774414, 2.9514031410217285, 2.965082883834839, 3.218356132507324, 3.2856030464172363, 2.5234341621398926, 2.8770627975463867, 2.6191389560699463, 3.125389814376831, 3.543398380279541, 2.8423361778259277, 2.0446979999542236, 2.702136516571045, 3.4541993141174316, 2.947829246520996, 3.3966031074523926, 2.7653698921203613, 3.795637607574463, 3.3281092643737793, 3.7443575859069824, 2.878854513168335, 3.820699691772461, 2.878758430480957, 3.2802932262420654, 4.285663604736328, 2.563904285430908, 4.110161781311035, 4.485726356506348, 3.3735647201538086, 2.6641268730163574, 3.220952033996582, 3.400418758392334, 3.22044038772583, 3.3324215412139893, 3.229726791381836, 4.124174118041992, 2.6323513984680176, 3.976719379425049, 4.277213096618652, 4.025479793548584, 3.959846258163452, 3.3607027530670166, 3.3026084899902344, 3.229508399963379, 3.423197031021118, 3.3978018760681152, 4.81397819519043, 2.2121152877807617, 2.5806827545166016, 2.980447292327881, 4.007701873779297, 5.214282989501953, 3.837810516357422, 4.686253547668457, 3.690242290496826, 3.7757272720336914, 3.5215654373168945, 2.951328992843628, 4.229669570922852, 3.9195210933685303, 4.317777633666992, 3.2801284790039062, 2.1458511352539062, 3.4444565773010254, 4.210400104522705, 3.548710346221924, 3.283576488494873, 3.8546106815338135, 3.1948041915893555, 4.062207221984863, 4.429303169250488, 2.7225871086120605, 4.202062606811523, 5.5866594314575195, 4.0868754386901855, 4.770808219909668, 4.441376686096191, 5.832900047302246, 6.123888969421387, 5.042385101318359, 3.4883439540863037, 5.646781921386719, 3.881054162979126, 3.5732789039611816, 2.675900936126709, 4.474611282348633, 3.393949270248413], 'val_acc': [0.1875, 0.1875, 0.3125, 0.25, 0.1875, 0.4375, 0.21875, 0.34375, 0.25, 0.375, 0.25, 0.3125, 0.21875, 0.53125, 0.40625, 0.5, 0.4375, 0.3125, 0.4375, 0.375, 0.4375, 0.3125, 0.3125, 0.46875, 0.46875, 0.4375, 0.34375, 0.28125, 0.34375, 0.375, 0.25, 0.4375, 0.40625, 0.40625, 0.25, 0.59375, 0.4375, 0.375, 0.4375, 0.34375, 0.40625, 0.3125, 0.40625, 0.375, 0.375, 0.375, 0.375, 0.40625, 0.375, 0.5625, 0.34375, 0.375, 0.34375, 0.4375, 0.5, 0.4375, 0.4375, 0.53125, 0.40625, 0.34375, 0.4375, 0.46875, 0.375, 0.375, 0.21875, 0.40625, 0.21875, 0.28125, 0.6153846383094788, 0.375, 0.46875, 0.40625, 0.46875, 0.375, 0.3125, 0.5625, 0.34375, 0.34375, 0.375, 0.59375, 0.40625, 0.4375, 0.46875, 0.21875, 0.5, 0.375, 0.28125, 0.375, 0.4375, 0.34375, 0.25, 0.15625, 0.1875, 0.53125, 0.40625, 0.53125, 0.4375, 0.59375, 0.34375, 0.34375], 'loss': [4.043189524704343, 3.904949864229804, 3.728587940296606, 3.585475546370477, 3.3961831896224655, 3.1846000522227467, 3.0087670270888753, 2.829178373271999, 2.6724904582983595, 2.551819732206614, 2.416864870628902, 2.3232247698951034, 2.179589454338612, 2.144047665410403, 2.020427248449251, 1.9292288083292228, 1.8571064441264251, 1.7760577147567742, 1.7049942009852275, 1.6805645768781348, 1.6096283525024615, 1.5292109889954884, 1.518817827087946, 1.4426144723988952, 1.4136361089847256, 1.381827161035945, 1.3467683070784162, 1.2834543052161744, 1.2432164687766138, 1.2431322029947982, 1.2385965192843844, 1.1894650732354473, 1.1338859984097995, 1.1040223708521733, 1.102327008943677, 1.064406594315574, 1.0719712957173875, 1.0586057386889889, 1.0019630416436813, 0.9775675097904444, 0.997928144881736, 0.9423311316874208, 0.9206998820123765, 0.9370765673942683, 0.8680855691447014, 0.9013137496510438, 0.8306775537125484, 0.8520551498106669, 0.841640803717757, 0.8168713435808368, 0.7994242843014644, 0.8042006565028976, 0.8086708903622025, 0.795676726810098, 0.7670031299172608, 0.7751499540008838, 0.7304322067986515, 0.7438420846963384, 0.7086906430293266, 0.7267062669375168, 0.6989375321081602, 0.669125519951269, 0.6614433903968624, 0.6721076307768641, 0.6807035537760447, 0.6618476880836892, 0.6759665742829375, 0.6544587264985626, 0.6446653783293259, 0.6132978363879645, 0.6664776462348854, 0.6388033609270459, 0.6218370443302721, 0.6244635203166456, 0.5775490741834912, 0.5818770272351008, 0.5829437677380498, 0.6068892578494861, 0.5787711765552979, 0.575637280434813, 0.565372576597418, 0.5763417080934106, 0.5843262563732666, 0.573249950340599, 0.544568855168685, 0.5870598361834684, 0.5436081021187524, 0.5385011408893701, 0.5285024402571276, 0.5465881076166162, 0.5412527061656107, 0.5104807784820846, 0.5349528586268284, 0.5669763877049289, 0.5198862692479621, 0.5170029906288411, 0.5033786970999872, 0.5230075817789832, 0.5016691014153071, 0.5199945199163378], 'acc': [0.14012738855260812, 0.1547534796188507, 0.188016041529772, 0.20842179759728738, 0.24593064406902954, 0.28155225293904607, 0.30702995991026294, 0.3438310922668576, 0.3730832743852817, 0.39384288757188785, 0.41873083277872414, 0.4457419202923353, 0.4721632460696879, 0.47617362587624606, 0.5054258079806092, 0.5224109459637639, 0.5359754659117717, 0.5450577967204584, 0.5714791223923534, 0.5764331211034743, 0.5881104033689055, 0.6039160178865738, 0.6108752063884857, 0.6246756310169358, 0.6322245812691673, 0.6455531966070376, 0.6468506723986844, 0.66489738144383, 0.677518282599763, 0.6736258552388836, 0.6769285209197485, 0.6837697570744087, 0.6915546119086555, 0.7029959895780103, 0.7062986553854244, 0.7147912243453645, 0.7172682235954674, 0.7179759377774048, 0.72965322009908, 0.7311866005992912, 0.7298891248779494, 0.7401509789904211, 0.754069355938001, 0.7485255957015367, 0.7609105920645391, 0.7502948809243734, 0.7735314933048383, 0.7645671148293421, 0.7704647322903548, 0.7773059683184659, 0.777659825388343, 0.7768341590278861, 0.7767162066946954, 0.7809624910973298, 0.7903986790040158, 0.7915782024624719, 0.7970040104219897, 0.794762915711719, 0.8016041519788674, 0.7966501533802345, 0.8044350082847863, 0.8168200046477888, 0.8187072422038162, 0.8137532437176714, 0.8135173389528629, 0.8165840999392244, 0.8112762443269584, 0.8244869073316381, 0.8224817173158709, 0.835102618570231, 0.8170559093282312, 0.8196508609677688, 0.8271998113184275, 0.8254305261096518, 0.8406463787785733, 0.838759141349095, 0.8360462373341837, 0.8343949045148428, 0.8417079499319607, 0.841589997570648, 0.8446567586976196, 0.8368719036665185, 0.8366359990985642, 0.8418259023635785, 0.8461901391978308, 0.8373437131820746, 0.8467799009340893, 0.8504364237129534, 0.8560981363810355, 0.8468978531829139, 0.8503184712532135, 0.8557442793252193, 0.8486671384901167, 0.8460721867521519, 0.857867421463262, 0.8570417550606221, 0.8569238027977364, 0.847723519726164, 0.8611700872144319, 0.8577494692425593]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O-Tr7zwz_9a",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing Class-Wise Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb3WFgKczO1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the class labels for the model\n",
        "classes = train_gen.class_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DIvGliF4qts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making wrapper folders in order to evaluate class-wise accuracy\n",
        "# for each pokemon in the dataset\n",
        "path = 'original_data_clean/validation'\n",
        "for pokemon in os.listdir(path):\n",
        "  if pokemon[0] != '.':\n",
        "\n",
        "    # creating a wrapper folder in the directory\n",
        "    wrapper_dir = os.path.join(path, \"wrapper_{}\".format(pokemon))\n",
        "    os.mkdir(wrapper_dir)\n",
        "\n",
        "    # moving the current folder to inside the wrapper folder\n",
        "    cur_dir = os.path.join(path, pokemon)\n",
        "    new_dir = os.path.join(wrapper_dir, pokemon)\n",
        "    os.rename(cur_dir, new_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsVUz3n1tHPB",
        "colab_type": "code",
        "outputId": "a7ee3883-ecca-4292-dcd6-e87aaef26255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# evaluating class-wise accuracy\n",
        "accuracy = {}\n",
        "\n",
        "# for each pokemon in the directory\n",
        "for pokemon in os.listdir(path):\n",
        "  if pokemon[0] != '.':\n",
        "\n",
        "    # creating a data generator for just that class\n",
        "    test_gen = test_datagen.flow_from_directory(path + '/{}'.format(pokemon),\n",
        "                                            target_size=(224, 224),\n",
        "                                            color_mode = \"rgb\",\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='categorical', \n",
        "                                            shuffle=True, seed=333)\n",
        "    \n",
        "    # getting the output of the model on that class\n",
        "    output = model.predict_generator(test_gen, workers=4, \n",
        "                                     use_multiprocessing=True, verbose=1)\n",
        "    \n",
        "    # getting prediction from class outputs\n",
        "    preds = np.argmax(output, axis=1)\n",
        "\n",
        "    # computing number of correct predictions\n",
        "    pokemon_name = pokemon.replace(\"wrapper_\", \"\")\n",
        "    correct = np.count_nonzero(preds == classes[pokemon_name])\n",
        "\n",
        "    # calculating accuracy as number of correct predictions/ total predictions\n",
        "    acc = correct / np.size(preds)\n",
        "\n",
        "    # saving accuracy in the dictionary\n",
        "    accuracy[pokemon] = acc\n",
        "\n",
        "    # printing results\n",
        "    print(\"{}: {}. Correct = {}, Total = {}\".format(pokemon, acc, correct, \n",
        "                                                    np.size(preds)))\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 283ms/step\n",
            "wrapper_Nidorino: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 1s 720ms/step\n",
            "wrapper_Poliwhirl: 0.23076923076923078. Correct = 3, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 280ms/step\n",
            "wrapper_Charizard: 0.36363636363636365. Correct = 4, Total = 11\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 320ms/step\n",
            "wrapper_Nidoqueen: 0.14285714285714285. Correct = 2, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 318ms/step\n",
            "wrapper_Rhydon: 0.4166666666666667. Correct = 5, Total = 12\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 345ms/step\n",
            "wrapper_Lickitung: 0.5714285714285714. Correct = 8, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 316ms/step\n",
            "wrapper_Nidorina: 0.4166666666666667. Correct = 5, Total = 12\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 369ms/step\n",
            "wrapper_Dragonite: 0.23076923076923078. Correct = 3, Total = 13\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 415ms/step\n",
            "wrapper_Farfetchd: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 335ms/step\n",
            "wrapper_Venomoth: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "wrapper_Paras: 0.2727272727272727. Correct = 3, Total = 11\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 313ms/step\n",
            "wrapper_Arbok: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 22 images belonging to 1 classes.\n",
            "1/1 [==============================] - 1s 518ms/step\n",
            "wrapper_Dratini: 0.5454545454545454. Correct = 12, Total = 22\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 292ms/step\n",
            "wrapper_Hitmonlee: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 301ms/step\n",
            "wrapper_Exeggcute: 0.25. Correct = 3, Total = 12\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 394ms/step\n",
            "wrapper_Mew: 0.35714285714285715. Correct = 5, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 349ms/step\n",
            "wrapper_Cubone: 0.0. Correct = 0, Total = 12\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 378ms/step\n",
            "wrapper_Jynx: 0.5. Correct = 6, Total = 12\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 433ms/step\n",
            "wrapper_MrMime: 0.4166666666666667. Correct = 5, Total = 12\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 316ms/step\n",
            "wrapper_Slowbro: 0.07692307692307693. Correct = 1, Total = 13\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 304ms/step\n",
            "wrapper_Dewgong: 0.14285714285714285. Correct = 2, Total = 14\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 294ms/step\n",
            "wrapper_Golem: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 268ms/step\n",
            "wrapper_Ekans: 0.2727272727272727. Correct = 3, Total = 11\n",
            "Found 10 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 267ms/step\n",
            "wrapper_Caterpie: 0.3. Correct = 3, Total = 10\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 1s 530ms/step\n",
            "wrapper_Grimer: 0.0. Correct = 0, Total = 13\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 307ms/step\n",
            "wrapper_Tangela: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 388ms/step\n",
            "wrapper_Nidoking: 0.14285714285714285. Correct = 2, Total = 14\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "wrapper_Kakuna: 0.5. Correct = 7, Total = 14\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Wartortle: 0.5384615384615384. Correct = 7, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Electabuzz: 0.7272727272727273. Correct = 8, Total = 11\n",
            "Found 15 images belonging to 1 classes.\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "wrapper_Pidgey: 0.26666666666666666. Correct = 4, Total = 15\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "wrapper_Tauros: 0.6428571428571429. Correct = 9, Total = 14\n",
            "Found 32 images belonging to 1 classes.\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "wrapper_Psyduck: 0.59375. Correct = 19, Total = 32\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Dodrio: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 58 images belonging to 1 classes.\n",
            "2/2 [==============================] - 14s 7s/step\n",
            "wrapper_Bulbasaur: 0.7758620689655172. Correct = 45, Total = 58\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "wrapper_Exeggutor: 0.5714285714285714. Correct = 8, Total = 14\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Parasect: 0.5454545454545454. Correct = 6, Total = 11\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Ponyta: 0.2857142857142857. Correct = 4, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Persian: 0.16666666666666666. Correct = 2, Total = 12\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Slowpoke: 0.08333333333333333. Correct = 1, Total = 12\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Meowth: 0.35714285714285715. Correct = 5, Total = 14\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Growlithe: 0.35714285714285715. Correct = 5, Total = 14\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Omastar: 0.45454545454545453. Correct = 5, Total = 11\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Poliwrath: 0.23076923076923078. Correct = 3, Total = 13\n",
            "Found 15 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Lapras: 0.4. Correct = 6, Total = 15\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Oddish: 0.35714285714285715. Correct = 5, Total = 14\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Vulpix: 0.21428571428571427. Correct = 3, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Geodude: 0.25. Correct = 3, Total = 12\n",
            "Found 10 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Ditto: 0.2. Correct = 2, Total = 10\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Machoke: 0.09090909090909091. Correct = 1, Total = 11\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Raticate: 0.21428571428571427. Correct = 3, Total = 14\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Dugtrio: 0.3076923076923077. Correct = 4, Total = 13\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Tentacool: 0.4166666666666667. Correct = 5, Total = 12\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "wrapper_Horsea: 0.07692307692307693. Correct = 1, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Porygon: 0.36363636363636365. Correct = 4, Total = 11\n",
            "Found 20 images belonging to 1 classes.\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "wrapper_Aerodactyl: 0.4. Correct = 8, Total = 20\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Pidgeot: 0.14285714285714285. Correct = 2, Total = 14\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Haunter: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Golduck: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Wigglytuff: 0.5. Correct = 7, Total = 14\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Kabutops: 0.35714285714285715. Correct = 5, Total = 14\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Koffing: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Machop: 0.18181818181818182. Correct = 2, Total = 11\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Moltres: 0.23076923076923078. Correct = 3, Total = 13\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Rhyhorn: 0.21428571428571427. Correct = 3, Total = 14\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Seadra: 0.2857142857142857. Correct = 4, Total = 14\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Weezing: 0.23076923076923078. Correct = 3, Total = 13\n",
            "Found 24 images belonging to 1 classes.\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "wrapper_Fearow: 0.9166666666666666. Correct = 22, Total = 24\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Blastoise: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Pidgeotto: 0.3076923076923077. Correct = 4, Total = 13\n",
            "Found 62 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:989: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 12s 6s/step\n",
            "wrapper_Mewtwo: 0.8387096774193549. Correct = 52, Total = 62\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Golbat: 0.42857142857142855. Correct = 6, Total = 14\n",
            "Found 15 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Raichu: 0.3333333333333333. Correct = 5, Total = 15\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Kabuto: 0.4166666666666667. Correct = 5, Total = 12\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Hitmonchan: 0.23076923076923078. Correct = 3, Total = 13\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Electrode: 0.2857142857142857. Correct = 4, Total = 14\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Metapod: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Magmar: 0.16666666666666666. Correct = 2, Total = 12\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Starmie: 0.3333333333333333. Correct = 4, Total = 12\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Butterfree: 0.2857142857142857. Correct = 4, Total = 14\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Beedrill: 0.36363636363636365. Correct = 4, Total = 11\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Magnemite: 0.3333333333333333. Correct = 4, Total = 12\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Chansey: 0.75. Correct = 9, Total = 12\n",
            "Found 15 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Sandslash: 0.13333333333333333. Correct = 2, Total = 15\n",
            "Found 15 images belonging to 1 classes.\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "wrapper_Ninetales: 0.13333333333333333. Correct = 2, Total = 15\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Kangaskhan: 0.23076923076923078. Correct = 3, Total = 13\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Drowzee: 0.4166666666666667. Correct = 5, Total = 12\n",
            "Found 60 images belonging to 1 classes.\n",
            "2/2 [==============================] - 14s 7s/step\n",
            "wrapper_Charmander: 0.7333333333333333. Correct = 44, Total = 60\n",
            "Found 60 images belonging to 1 classes.\n",
            "2/2 [==============================] - 12s 6s/step\n",
            "wrapper_Pikachu: 0.8. Correct = 48, Total = 60\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Venonat: 0.25. Correct = 3, Total = 12\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Vaporeon: 0.5714285714285714. Correct = 8, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Staryu: 0.3333333333333333. Correct = 4, Total = 12\n",
            "Found 56 images belonging to 1 classes.\n",
            "2/2 [==============================] - 1s 575ms/step\n",
            "wrapper_Squirtle: 0.6964285714285714. Correct = 39, Total = 56\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Ivysaur: 0.5454545454545454. Correct = 6, Total = 11\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Krabby: 0.23076923076923078. Correct = 3, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Victreebel: 0.18181818181818182. Correct = 2, Total = 11\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Dragonair: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Gengar: 0.4166666666666667. Correct = 5, Total = 12\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Voltorb: 0.3076923076923077. Correct = 4, Total = 13\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Cloyster: 0.4166666666666667. Correct = 5, Total = 12\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Onix: 0.07692307692307693. Correct = 1, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Weepinbell: 0.0. Correct = 0, Total = 11\n",
            "Found 10 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Gastly: 0.7. Correct = 7, Total = 10\n",
            "Found 15 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Mankey: 0.3333333333333333. Correct = 5, Total = 15\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Rattata: 0.5714285714285714. Correct = 8, Total = 14\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Diglett: 0.09090909090909091. Correct = 1, Total = 11\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Primeape: 0.14285714285714285. Correct = 2, Total = 14\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Scyther: 0.2857142857142857. Correct = 4, Total = 14\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Snorlax: 0.5714285714285714. Correct = 8, Total = 14\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Sandshrew: 0.2857142857142857. Correct = 4, Total = 14\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Marowak: 0.07142857142857142. Correct = 1, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Articuno: 0.25. Correct = 3, Total = 12\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Gloom: 0.4166666666666667. Correct = 5, Total = 12\n",
            "Found 26 images belonging to 1 classes.\n",
            "1/1 [==============================] - 9s 9s/step\n",
            "wrapper_Spearow: 0.8846153846153846. Correct = 23, Total = 26\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Clefairy: 0.5. Correct = 6, Total = 12\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Charmeleon: 0.3076923076923077. Correct = 4, Total = 13\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Arcanine: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Tentacruel: 0.0. Correct = 0, Total = 11\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Vileplume: 0.35714285714285715. Correct = 5, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Weedle: 0.16666666666666666. Correct = 2, Total = 12\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Venusaur: 0.5. Correct = 7, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Goldeen: 0.16666666666666666. Correct = 2, Total = 12\n",
            "Found 15 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Muk: 0.06666666666666667. Correct = 1, Total = 15\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Pinsir: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 10 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Doduo: 0.4. Correct = 4, Total = 10\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Magikarp: 0.08333333333333333. Correct = 1, Total = 12\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Seaking: 0.46153846153846156. Correct = 6, Total = 13\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Jigglypuff: 0.15384615384615385. Correct = 2, Total = 13\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Flareon: 0.25. Correct = 3, Total = 12\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Machamp: 0.2857142857142857. Correct = 4, Total = 14\n",
            "Found 17 images belonging to 1 classes.\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "wrapper_Rapidash: 0.23529411764705882. Correct = 4, Total = 17\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Poliwag: 0.23076923076923078. Correct = 3, Total = 13\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Gyarados: 0.5. Correct = 7, Total = 14\n",
            "Found 10 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Zubat: 0.2. Correct = 2, Total = 10\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Graveler: 0.2727272727272727. Correct = 3, Total = 11\n",
            "Found 14 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Kingler: 0.5. Correct = 7, Total = 14\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Magneton: 0.3333333333333333. Correct = 4, Total = 12\n",
            "Found 10 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Alakazam: 0.2. Correct = 2, Total = 10\n",
            "Found 10 images belonging to 1 classes.\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "wrapper_Clefable: 0.6. Correct = 6, Total = 10\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Seel: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Hypno: 0.3076923076923077. Correct = 4, Total = 13\n",
            "Found 9 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Eevee: 0.3333333333333333. Correct = 3, Total = 9\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Bellsprout: 0.45454545454545453. Correct = 5, Total = 11\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "wrapper_Kadabra: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 11 images belonging to 1 classes.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "wrapper_Omanyte: 0.18181818181818182. Correct = 2, Total = 11\n",
            "Found 13 images belonging to 1 classes.\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "wrapper_Jolteon: 0.38461538461538464. Correct = 5, Total = 13\n",
            "Found 17 images belonging to 1 classes.\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "wrapper_Shellder: 0.6470588235294118. Correct = 11, Total = 17\n",
            "Found 12 images belonging to 1 classes.\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "wrapper_Zapdos: 0.0. Correct = 0, Total = 12\n",
            "Found 9 images belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 369ms/step\n",
            "wrapper_Abra: 0.2222222222222222. Correct = 2, Total = 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aplv16mIOxk",
        "colab_type": "code",
        "outputId": "43cd5634-a0d2-49da-fa02-0d572b7709a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(accuracy.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([('wrapper_Nidorino', 0.38461538461538464), ('wrapper_Poliwhirl', 0.23076923076923078), ('wrapper_Charizard', 0.36363636363636365), ('wrapper_Nidoqueen', 0.14285714285714285), ('wrapper_Rhydon', 0.4166666666666667), ('wrapper_Lickitung', 0.5714285714285714), ('wrapper_Nidorina', 0.4166666666666667), ('wrapper_Dragonite', 0.23076923076923078), ('wrapper_Farfetchd', 0.46153846153846156), ('wrapper_Venomoth', 0.46153846153846156), ('wrapper_Paras', 0.2727272727272727), ('wrapper_Arbok', 0.46153846153846156), ('wrapper_Dratini', 0.5454545454545454), ('wrapper_Hitmonlee', 0.46153846153846156), ('wrapper_Exeggcute', 0.25), ('wrapper_Mew', 0.35714285714285715), ('wrapper_Cubone', 0.0), ('wrapper_Jynx', 0.5), ('wrapper_MrMime', 0.4166666666666667), ('wrapper_Slowbro', 0.07692307692307693), ('wrapper_Dewgong', 0.14285714285714285), ('wrapper_Golem', 0.46153846153846156), ('wrapper_Ekans', 0.2727272727272727), ('wrapper_Caterpie', 0.3), ('wrapper_Grimer', 0.0), ('wrapper_Tangela', 0.38461538461538464), ('wrapper_Nidoking', 0.14285714285714285), ('wrapper_Kakuna', 0.5), ('wrapper_Wartortle', 0.5384615384615384), ('wrapper_Electabuzz', 0.7272727272727273), ('wrapper_Pidgey', 0.26666666666666666), ('wrapper_Tauros', 0.6428571428571429), ('wrapper_Psyduck', 0.59375), ('wrapper_Dodrio', 0.38461538461538464), ('wrapper_Bulbasaur', 0.7758620689655172), ('wrapper_Exeggutor', 0.5714285714285714), ('wrapper_Parasect', 0.5454545454545454), ('wrapper_Ponyta', 0.2857142857142857), ('wrapper_Persian', 0.16666666666666666), ('wrapper_Slowpoke', 0.08333333333333333), ('wrapper_Meowth', 0.35714285714285715), ('wrapper_Growlithe', 0.35714285714285715), ('wrapper_Omastar', 0.45454545454545453), ('wrapper_Poliwrath', 0.23076923076923078), ('wrapper_Lapras', 0.4), ('wrapper_Oddish', 0.35714285714285715), ('wrapper_Vulpix', 0.21428571428571427), ('wrapper_Geodude', 0.25), ('wrapper_Ditto', 0.2), ('wrapper_Machoke', 0.09090909090909091), ('wrapper_Raticate', 0.21428571428571427), ('wrapper_Dugtrio', 0.3076923076923077), ('wrapper_Tentacool', 0.4166666666666667), ('wrapper_Horsea', 0.07692307692307693), ('wrapper_Porygon', 0.36363636363636365), ('wrapper_Aerodactyl', 0.4), ('wrapper_Pidgeot', 0.14285714285714285), ('wrapper_Haunter', 0.38461538461538464), ('wrapper_Golduck', 0.38461538461538464), ('wrapper_Wigglytuff', 0.5), ('wrapper_Kabutops', 0.35714285714285715), ('wrapper_Koffing', 0.38461538461538464), ('wrapper_Machop', 0.18181818181818182), ('wrapper_Moltres', 0.23076923076923078), ('wrapper_Rhyhorn', 0.21428571428571427), ('wrapper_Seadra', 0.2857142857142857), ('wrapper_Weezing', 0.23076923076923078), ('wrapper_Fearow', 0.9166666666666666), ('wrapper_Blastoise', 0.46153846153846156), ('wrapper_Pidgeotto', 0.3076923076923077), ('wrapper_Mewtwo', 0.8387096774193549), ('wrapper_Golbat', 0.42857142857142855), ('wrapper_Raichu', 0.3333333333333333), ('wrapper_Kabuto', 0.4166666666666667), ('wrapper_Hitmonchan', 0.23076923076923078), ('wrapper_Electrode', 0.2857142857142857), ('wrapper_Metapod', 0.46153846153846156), ('wrapper_Magmar', 0.16666666666666666), ('wrapper_Starmie', 0.3333333333333333), ('wrapper_Butterfree', 0.2857142857142857), ('wrapper_Beedrill', 0.36363636363636365), ('wrapper_Magnemite', 0.3333333333333333), ('wrapper_Chansey', 0.75), ('wrapper_Sandslash', 0.13333333333333333), ('wrapper_Ninetales', 0.13333333333333333), ('wrapper_Kangaskhan', 0.23076923076923078), ('wrapper_Drowzee', 0.4166666666666667), ('wrapper_Charmander', 0.7333333333333333), ('wrapper_Pikachu', 0.8), ('wrapper_Venonat', 0.25), ('wrapper_Vaporeon', 0.5714285714285714), ('wrapper_Staryu', 0.3333333333333333), ('wrapper_Squirtle', 0.6964285714285714), ('wrapper_Ivysaur', 0.5454545454545454), ('wrapper_Krabby', 0.23076923076923078), ('wrapper_Victreebel', 0.18181818181818182), ('wrapper_Dragonair', 0.46153846153846156), ('wrapper_Gengar', 0.4166666666666667), ('wrapper_Voltorb', 0.3076923076923077), ('wrapper_Cloyster', 0.4166666666666667), ('wrapper_Onix', 0.07692307692307693), ('wrapper_Weepinbell', 0.0), ('wrapper_Gastly', 0.7), ('wrapper_Mankey', 0.3333333333333333), ('wrapper_Rattata', 0.5714285714285714), ('wrapper_Diglett', 0.09090909090909091), ('wrapper_Primeape', 0.14285714285714285), ('wrapper_Scyther', 0.2857142857142857), ('wrapper_Snorlax', 0.5714285714285714), ('wrapper_Sandshrew', 0.2857142857142857), ('wrapper_Marowak', 0.07142857142857142), ('wrapper_Articuno', 0.25), ('wrapper_Gloom', 0.4166666666666667), ('wrapper_Spearow', 0.8846153846153846), ('wrapper_Clefairy', 0.5), ('wrapper_Charmeleon', 0.3076923076923077), ('wrapper_Arcanine', 0.46153846153846156), ('wrapper_Tentacruel', 0.0), ('wrapper_Vileplume', 0.35714285714285715), ('wrapper_Weedle', 0.16666666666666666), ('wrapper_Venusaur', 0.5), ('wrapper_Goldeen', 0.16666666666666666), ('wrapper_Muk', 0.06666666666666667), ('wrapper_Pinsir', 0.38461538461538464), ('wrapper_Doduo', 0.4), ('wrapper_Magikarp', 0.08333333333333333), ('wrapper_Seaking', 0.46153846153846156), ('wrapper_Jigglypuff', 0.15384615384615385), ('wrapper_Flareon', 0.25), ('wrapper_Machamp', 0.2857142857142857), ('wrapper_Rapidash', 0.23529411764705882), ('wrapper_Poliwag', 0.23076923076923078), ('wrapper_Gyarados', 0.5), ('wrapper_Zubat', 0.2), ('wrapper_Graveler', 0.2727272727272727), ('wrapper_Kingler', 0.5), ('wrapper_Magneton', 0.3333333333333333), ('wrapper_Alakazam', 0.2), ('wrapper_Clefable', 0.6), ('wrapper_Seel', 0.38461538461538464), ('wrapper_Hypno', 0.3076923076923077), ('wrapper_Eevee', 0.3333333333333333), ('wrapper_Bellsprout', 0.45454545454545453), ('wrapper_Kadabra', 0.38461538461538464), ('wrapper_Omanyte', 0.18181818181818182), ('wrapper_Jolteon', 0.38461538461538464), ('wrapper_Shellder', 0.6470588235294118), ('wrapper_Zapdos', 0.0), ('wrapper_Abra', 0.2222222222222222)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALuRqfDkvHPN",
        "colab_type": "code",
        "outputId": "4f49f5e9-e8a9-4d58-a67a-c4adbf2a16eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "np.histogram(list(accuracy.values()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([14, 15, 26, 29, 25, 21,  7,  5,  4,  3]),\n",
              " array([0.        , 0.09166667, 0.18333333, 0.275     , 0.36666667,\n",
              "        0.45833333, 0.55      , 0.64166667, 0.73333333, 0.825     ,\n",
              "        0.91666667]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BnLGpZSv-Qd",
        "colab_type": "code",
        "outputId": "89d76f0e-bbab-4fed-dbe4-94b870342b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.hist(list(accuracy.values()), bins=10, histtype='bar')\n",
        "plt.xlabel(\"Class-Wise Test Accuracy Ranges\")\n",
        "plt.ylabel(\"Number of Pokemon\")\n",
        "plt.title(\"Distribution of Class-Wise Test Accuracy\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Distribution of Class-Wise Test Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMIY5teEH1uv",
        "colab_type": "text"
      },
      "source": [
        "### Exploring link Between Number of Training Image and Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvoKVYKjC4sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a dictionary with number of training examples per pokemon\n",
        "counts = {}\n",
        "for pokemon in os.listdir('original_data_clean/train'):\n",
        "    if pokemon[0] != '.':\n",
        "      name = pokemon.replace(\"wrapper_\", \"\")\n",
        "      counts[name] = len(os.listdir('original_data_clean/train/{}'.format(pokemon)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUrYNSmODmeA",
        "colab_type": "code",
        "outputId": "4826bedd-1b2b-46d8-b6b8-46c934df8fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(counts.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([('Nidoqueen', 51), ('Farfetchd', 51), ('Poliwhirl', 51), ('Charizard', 41), ('Nidorina', 47), ('Lickitung', 53), ('Nidorino', 50), ('Arbok', 50), ('Venomoth', 52), ('Exeggcute', 45), ('Cubone', 47), ('Dratini', 87), ('Dragonite', 49), ('Rhydon', 47), ('Hitmonlee', 52), ('Paras', 44), ('Mew', 53), ('Kakuna', 53), ('Golem', 51), ('Caterpie', 39), ('MrMime', 47), ('Grimer', 51), ('Jynx', 47), ('Slowbro', 50), ('Tangela', 50), ('Ekans', 41), ('Nidoking', 54), ('Wartortle', 48), ('Dewgong', 52), ('Parasect', 42), ('Electabuzz', 42), ('Exeggutor', 54), ('Pidgey', 59), ('Persian', 45), ('Tauros', 53), ('Dodrio', 52), ('Meowth', 56), ('Psyduck', 127), ('Ponyta', 52), ('Omastar', 45), ('Bulbasaur', 231), ('Horsea', 50), ('Slowpoke', 48), ('Oddish', 52), ('Machoke', 40), ('Poliwrath', 50), ('Growlithe', 54), ('Lapras', 56), ('Tentacool', 45), ('Geodude', 44), ('Vulpix', 54), ('Pidgeot', 52), ('Porygon', 44), ('Ditto', 39), ('Koffing', 52), ('Wigglytuff', 53), ('Dugtrio', 51), ('Machop', 42), ('Raticate', 56), ('Haunter', 50), ('Aerodactyl', 77), ('Golbat', 53), ('Moltres', 49), ('Rhyhorn', 55), ('Hitmonchan', 48), ('Blastoise', 49), ('Seadra', 53), ('Golduck', 47), ('Weezing', 50), ('Kabutops', 51), ('Pidgeotto', 50), ('Kabuto', 44), ('Magmar', 47), ('Electrode', 53), ('Fearow', 98), ('Beedrill', 42), ('Magnemite', 48), ('Metapod', 52), ('Raichu', 56), ('Mewtwo', 244), ('Ninetales', 59), ('Starmie', 47), ('Butterfree', 52), ('Staryu', 48), ('Vaporeon', 54), ('Chansey', 46), ('Kangaskhan', 50), ('Sandslash', 56), ('Ivysaur', 42), ('Venonat', 45), ('Krabby', 51), ('Drowzee', 48), ('Charmander', 236), ('Victreebel', 44), ('Pikachu', 238), ('Dragonair', 52), ('Rattata', 53), ('Gengar', 48), ('Mankey', 57), ('Cloyster', 47), ('Gastly', 40), ('Weepinbell', 40), ('Voltorb', 52), ('Squirtle', 224), ('Onix', 51), ('Diglett', 40), ('Primeape', 56), ('Charmeleon', 52), ('Spearow', 104), ('Sandshrew', 52), ('Snorlax', 55), ('Clefairy', 48), ('Articuno', 44), ('Scyther', 53), ('Marowak', 56), ('Gloom', 46), ('Tentacruel', 45), ('Goldeen', 46), ('Doduo', 38), ('Weedle', 47), ('Venusaur', 52), ('Jigglypuff', 52), ('Vileplume', 53), ('Magikarp', 47), ('Arcanine', 48), ('Flareon', 47), ('Seaking', 52), ('Muk', 56), ('Pinsir', 50), ('Machamp', 57), ('Poliwag', 51), ('Kingler', 54), ('Magneton', 46), ('Rapidash', 66), ('Abra', 33), ('Gyarados', 53), ('Clefable', 39), ('Alakazam', 39), ('Graveler', 46), ('Zubat', 39), ('Seel', 48), ('Hypno', 49), ('Omanyte', 44), ('Shellder', 64), ('Jolteon', 51), ('Kadabra', 48), ('Zapdos', 47), ('Eevee', 32), ('Bellsprout', 44)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3bPiFKfszwL",
        "colab_type": "code",
        "outputId": "00e90231-336f-4d36-9c41-471e2d1faa72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "# plotting number of training examples vs accuracy\n",
        "acc, freq = [], []\n",
        "\n",
        "for pokemon in os.listdir('original_data_clean/validation'):\n",
        "  if pokemon[0] != '.':\n",
        "    name = pokemon.replace(\"wrapper_\", \"\")\n",
        "    freq.append(counts[name])\n",
        "    acc.append(accuracy[pokemon] * 100)\n",
        "\n",
        "plt.scatter(freq, acc, c='purple')\n",
        "plt.xlabel(\"Number of Training Examples\")\n",
        "plt.ylabel(\"Test Accuracy (percentage)\")\n",
        "plt.title(\"Number of Training Examples vs. Test Accuracy\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Number of Training Examples vs. Test Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEWCAYAAACAOivfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7tCA7svFVkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating the pearson correlation\n",
        "from scipy.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APGghwMyFZh2",
        "colab_type": "code",
        "outputId": "883ff254-ca01-4851-cca2-8f9ea8ef590a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pearsonr(freq, acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4915997596931797, 1.935458418232572e-10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    }
  ]
}